---
title: "Aoa_prediction_reliability_morphosyntax"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
options(dplyr.summarise.inform = FALSE)

library(tidyverse)
#library(profvis)
library(glue)
library(wordbankr)
library(childesr)
#library(feather)

normalize_language <- function(language) {
  language %>% str_replace(" ", "_") %>% str_to_lower()
}

walk(list.files("scripts", pattern = "*.R", full.names = TRUE), source)

#target_langs <- c("Russian", "Turkish", "Croatian", "Danish", "Italian")
target_langs <- c("Italian", "Norwegian", "Russian", "Turkish", "Danish")

childes_path <- "/Users/lscpuser/Desktop/emerge2/aoa-pipeline/data/childes/"

```


### Load saved CHILDES .csv corpus for a language (here French and Italian). 

Rbind different corpora.

```{r aoa}

load_wordbank<- function(lang) {
  words <- get_inst_words(lang, "WS")
  admins <- get_inst_admins(lang, "WS")
  items <- get_instrument_data(language = lang,
                                 items = words$item_id,
                                 form = "WS",
                                 administrations = admins,
                                 iteminfo = words) %>%
                                 rename(lemma = definition) 
  return(items)
}

get_aoas <- function(lang){
  items <- load_wordbank(lang)
  aoa <- fit_aoa(items, measure = "produces", method = "glmrob", proportion = 0.5)%>%
                                 filter(!is.na(aoa)) %>%
                                  mutate(language=lang)
  return(aoa)
}
```

### Load CHILDES predictors. 

```{r prep_predictors, message=FALSE}

predictors <- map_dfr(target_langs, function(lang) {
  norm_lang <- normalize_language(lang)
  file_ <- file.path(childes_path, glue("unilemma_metrics_{norm_lang}.csv"))

  if(!file.exists(file_))
  {  
  print(glue("{file_} doesn't exist. Retrieving data..."))
  childes_predictors<- get_childes_metrics(lang)
  }
  read_csv(glue(childes_path, "unilemma_metrics_{norm_lang}.csv"))
}) #####  English error while fetching rowsOnly first 4,138,918 results retrieved. Use n = Inf to retrieve all.

#summary<-function(predictor_data){
#predictors <- predictor_data %>% 
#              distinct(uni_lemma, language, mlu_word, frequency_word, solo_word, initial_word,
#           final_word, nb_realisations_lemma, mean_character_count_lemma,
#           totalcount) %>%
#              group_by(language, uni_lemma) %>%
#                summarise(freq = frequency/totalcount, 
#                        solo_freq = solo_frequency/frequency,
#                        final_freq = final_frequency/frequency,
#                        init_freq = initial_frequency/frequency,
#                        mlu = MLU,
#                        char_number = mean_character_count_lemma,
#                        nb_forms = nb_realisations_lemma)
#return(predictors)
#}
```

### Measure aoas

```{r get_aoas}

aoas <- lapply(target_langs, get_aoas) %>%
  bind_rows()

lang_data <- map_df(target_langs, function(lang) {
  lang_str <- normalize_language(lang)
  readRDS(glue("/Users/lscpuser/Desktop/emerge2/aoa-pipeline/data/wordbank/{lang_str}.rds"))
})

uni_lemmas <- lang_data %>%
  distinct(language, uni_lemma, items) %>%
  unnest(items) %>%
  select(-form) %>%
  left_join(aoas) %>%
  unique()

``` 


```{r merge_frequency_aoas}
# Merge aoas and predictors

```

### Plot frequency and aoa using log frequency and model intercept frequency
```{r plot}
#ggplot(d, 
#       aes(x=freq, y=aoa, label=uni_lemma)) + 
#    geom_smooth(method = "lm", formula = y~x) + 
#    geom_point(alpha=.1) +
#    facet_grid(lexical_class~language) +
#  scale_x_log10() + 
#  xlab("Frequency (log10)") + 
#  ylab("Age of Acquisition (months)") + 
#  theme_bw()

```
### Reliability_frequency: half-split and Spearman-Brown

```{r reliability}

# add lemmas of the first half not existing at the second half, with 1 
same_size_df <- function(df1, df2) { 
  firstlistlemma<-(df1$uni_lemma)
  secondlistlemma<-(df2$uni_lemma)
  diff1<-setdiff(firstlistlemma,secondlistlemma) 
  diff2<-setdiff(secondlistlemma,firstlistlemma) 
  df<-as.data.frame(c(diff1, diff2))
  colnames(df)<- c("uni_lemma")
  if (nrow(df> 0)){
    print("parts not equal")
    df1<- df1 %>%
      full_join(df) %>%
      arrange(uni_lemma)
    df2<- df2 %>%
      full_join(df)%>%
      arrange(uni_lemma)
  }else{}
  both=list(tibble(df1[!duplicated(df1$uni_lemma),]), tibble(df2[!duplicated(df2$uni_lemma),]))
  return(both)
}

######################
unilemma_half_cor<- function(half){  # count raw frequency of half #word_class
  half_ <- half %>%
  mutate(stem = stem(word, convert_lang_stemmer(lang))) %>%
  full_join(load_unilemmas(uni_lemmas) %>% filter(language == normalize_language(lang)), by ="stem") %>%
  group_by(uni_lemma) %>%
  mutate(language=lang) %>%  
  summarise(nb_forms=n(),
              words_lemma = list(unique(word)),
              #sum_wordcount_lemma = sum(wordcount, na.rm = TRUE),
              char_number = mean(charactercount, na.rm = TRUE), 
              frequency=sum(frequency_word, na.rm =TRUE),
              mlu=mean(mlu_word, na.rm = TRUE),
              solo_frequency=sum(solo_word, na.rm = TRUE),
              final_frequency=sum(final_word, na.rm = TRUE),     
              initial_frequency=sum(initial_word, na.rm = TRUE),
              freq = frequency/totalcount, 
              solo_freq = solo_frequency/frequency,
              final_freq = final_frequency/frequency,
              init_freq = initial_frequency/frequency) %>%
  unique()
  half_ <- half_[!is.na(half_$freq), ]
  return(half_[order(half_$uni_lemma),])
}

#######################  
split_half_cor <-function(lang ){ #word_class
  lang_str <- normalize_language(lang)
  corpus <- readRDS(glue("/Users/lscpuser/Desktop/emerge2/aoa-pipeline/data/{lang}_tokens.rds"))
  #corpus <- get_tokens(language = convert_lang_childes(lang), token="*", role_exclude ="Target_Child")
  #utterances <- get_utterances(language = convert_lang_childes(lang), role_exclude ="Target_Child") %>%
       #                          mutate(gloss = tolower(gloss)) 
  utterances <- readRDS(glue("/Users/lscpuser/Desktop/emerge2/aoa-pipeline/data/{lang}_utterances.rds"))
  wb<- predictors %>% filter(language == lang)
  wblemmas<-unique(wb$word) #unique wordbank lemmas
  tokens_shrank <- corpus %>%
    filter(gloss %in% wblemmas)
  ind <- sample(c(TRUE, FALSE), nrow(tokens_shrank), replace=TRUE, prob=c(0.5, 0.5)) #split tokens
  tokens1 <- tokens_shrank[ind, ] #split in two
  tokens2 <- tokens_shrank[!ind, ] #split in two
  split_df<- function(tokens){
     utterances <- utterances %>%
     filter(id %in% tokens$utterance_id)
     half <- list(utterances = tibble(utterances), 
            tokens = tibble(tokens))
     half<- get_childes_metrics(lang=lang, half=half)
    return(half)
  }
  both=list(tibble(split_df(tokens1)), tibble(split_df(tokens2)))
  return(both)  
  }
  

#######################
sbformula <- function(r){  #adjust with spearman-brown formula
  r1<-(2*r)/(1+r)
  return(r1)
}

#######################
fix_nas <- function(df){
 df<-df %>%
   mutate(nb_forms=ifelse(is.na(nb_forms), "0", nb_forms)) %>%
  mutate(char_number=ifelse(is.na(char_number), "0", char_number)) %>%
  mutate(init_freq=ifelse(is.na(init_freq), "0", init_freq)) %>%
  mutate(solo_freq=ifelse(is.na(solo_freq), "0", solo_freq)) %>%
  mutate(final_freq=ifelse(is.na(final_freq), "0", final_freq)) %>%
  mutate(freq=ifelse(is.na(freq), "0", freq)) %>%
  mutate(mlu=ifelse(is.na(mlu), "0", mlu))
return(df)
}
########################

main_split_half <- function(lang){
both<-split_half_cor(lang)
df1<-unilemma_half_cor(both[[1]])
df2<-unilemma_half_cor(both[[2]])                    
df<- same_size_df(df1,df2)
df[[1]]<-fix_nas(df[[1]])
df[[2]]<-fix_nas(df[[2]])
return(df)
}

corr_split<-function(lang){
df <- main_split_half(lang)  
nb_forms_r=cor(as.numeric(df[[1]]$nb_forms), as.numeric(df[[2]]$nb_forms), method="pearson")
char_number_r=cor(as.numeric(df[[1]]$char_number), as.numeric(df[[2]]$char_number), method="pearson")
freq_r=cor(as.numeric(df[[1]]$freq), as.numeric(df[[2]]$freq), method="pearson")
init_freq_r=cor(as.numeric(df[[1]]$init_freq), as.numeric(df[[2]]$init_freq), method="pearson")
solo_freq_r=cor(as.numeric(df[[1]]$solo_freq), as.numeric(df[[2]]$solo_freq), method="pearson")
final_freq_r=cor(as.numeric(df[[1]]$final_freq), as.numeric(df[[2]]$final_freq), method="pearson")
mlu_r=cor(as.numeric(df[[1]]$mlu), as.numeric(df[[2]]$mlu), method="pearson")
r=c(nb_forms_r,char_number_r,freq_r,init_freq_r,solo_freq_r,final_freq_r,mlu_r)
names= c("nb_forms","char_number","freq","init_freq","solo_freq","final_freq","mlu")
rdata <- data.frame(names, r)
rdata <- rdata %>% mutate(language=lang)
return(r)
}


lang_pred_half <- function(lang, pred, split_half){
 s=split_half %>% filter(language==lang, names==pred)
 r=s[1,2]
return(as.character(r) ) 
}


``` 

### Reliability_frequency: cronbach alpha


### Measure reliabilities

```{r apply_reliability_frequency}

split_half <- lapply(target_langs, corr_split) %>%
  bind_rows() #####could not find croatianpy




reliabilities <- expand_grid(lang = c("Italian", "Norwegian", "Russian", "Turkish", "Danish"),
                            word_class = c("all"),
                            pred = c("nb_forms", "char_number", "freq", "inti_freq", "solo_freq", "final_freq", "mlu")) %>%  #"nouns","adjectives","verbs","function_words","other
                rowwise %>%            
#  mutate(corrr =  split_half %>% filter(language =language, predictor=names) %>% 
 mutate(corrr =  lang_pred_half(lang, pred, split_half)) %>% #split_half
  mutate(sbr = sbformula(as.numeric(corrr)))

reliabilities %>%
  knitr::kable(digits = 2)

#write_csv(reliabilities, file.path(childes_path, glue("reliabilities.csv")))
#reliabilities <-read_csv(glue(childes_path, "reliabilities.csv"))

```  

### Reliability_AoA

```{r reliability_aoa}  

split_half_cor_aoa <-function(lang_){
  #print(glue("Get CDI item data for {lang_} and {clas_} ..."))
  items <-load_wordbank(lang)
  admin<-as.data.frame(unique(items$data_id))
  
  print(glue("Randomly split administrations..."))
  ind <- sample(c(TRUE, FALSE), nrow(admin), replace=TRUE, prob=c(0.5, 0.5)) #randomly split administrations
  adminfirstnum <- admin[ind, ]
  adminsecondnum <- admin[!ind, ] #create two groups of administrations
  
  adminfirst<-items %>% filter(data_id %in% adminfirstnum) #filter items in administrations
  adminsecond<-items %>% filter(data_id %in% adminsecondnum)
  
  aoafirst<- fit_aoa(adminfirst, method = "glmrob", proportion = 0.5) # get aoa for each group
  aoasecond<- fit_aoa(adminsecond, method = "glmrob", proportion = 0.5) # 
  
  r<-cor(aoafirst$aoa, aoasecond$aoa, use="complete.obs", method="pearson") #measure r
  return(r)
}

#write_csv(reliabilities_aoa, file.path(childes_path, glue("reliabilities.csv")))
#reliabilities <-read_csv(glue(childes_path, "reliabilities.csv"))

```

```{r apply_reliability_aoa}

#file_ <- file.path(childes_path, glue("/reliabilities_aoa_{lang}.csv"))
#if(!file.exists(file_)){  
#  print(glue("{file_} doesn't exist. Measuring aoa reliabilities..."))
  
reliabilities_aoa <- expand_grid(language = c( "Italian", "Norwegian", "Russian", "Turkish", "Danish"),
                            word_class = c("all")) %>% # "nouns","adjectives","verbs",   "function_words","other"
  rowwise %>%
  mutate(split_half_aoa = ifelse(word_class == "all", 
                             split_half_cor_aoa(language),
                             split_half_cor_aoa(language, word_class)),
         split_half_aoa_sb = sbformula(split_half_aoa))

  #write_feather(reliabilities_aoa, file.path(childes_path, glue("reliabilities_aoa_{lang}.csv")))
#  } else{
#  reliabilities_aoa <- read_feather(file_)
#}

### Problem with `mutate()` column `split_half_aoa`.
### ℹ `split_half_aoa = ifelse(...)`.
### ℹ fitted probabilities numerically 0 or 1 occurred
### ℹ The warning occurred in group 375: num_item_id = 375. 

reliabilities_aoa %>%
  knitr::kable(digits = 2)


```

### Regression

```{r final_Data}
build_final_data <- function(lang){
predictor_data <- predictors %>% filter(language==lang)  %>%
    group_by(uni_lemma) %>%
    summarise(nb_forms=n(),
              words_lemma = list(unique(word)),
              #sum_wordcount_lemma = sum(wordcount, na.rm = TRUE),
              char_number = mean(charactercount, na.rm = TRUE), 
              frequency=sum(frequency_word, na.rm =TRUE),
              mlu=mean(mlu_word, na.rm = TRUE),
              solo_frequency=sum(solo_word, na.rm = TRUE),
              final_frequency=sum(final_word, na.rm = TRUE),     
              initial_frequency=sum(initial_word, na.rm = TRUE),
              freq = frequency/totalcount, 
              solo_freq = solo_frequency/frequency,
              final_freq = final_frequency/frequency,
              init_freq = initial_frequency/frequency) %>%
   unique() %>%
   ungroup() %>%
   select(uni_lemma, nb_forms,char_number, freq, mlu, solo_freq, final_freq, init_freq)

aoas <- aoas %>% filter(language== lang)
 d <- aoas %>%
  mutate(language=lang) %>%  
  left_join(predictor_data) %>%
  ungroup() %>% 
  select(-c(item_id, num_item_id)) %>%
  filter(!is.na(aoa), !is.na(uni_lemma), !is.na(freq))  %>%
  unique()

# # unnest() %>%
#  group_by(language, uni_lemma, lexical_class) %>%
#   summarise(aoa = aoa[1]) %>%
return(d)
}
```


```{r main_regression}

 regression_option1<-function(d,  pr){
 predictor <- d[[pr]]
 option1<-lm(aoa~predictor, d) 
 return(summary(option1)$adj.r.squared)
 }
# 
 r2 <- expand_grid(lang = c("Italian", "Norwegian", "Russian", "Turkish", "Danish"), 
              #     class = c("all"),
                   pred = c("freq",  "mlu", "final_freq", "solo_freq", "init_freq", "char_number")) %>% 
  rowwise %>%
  mutate(r2=regression_option1(d, pr = pred)) %>%
  rename( language = lang, predictor=pred)
  #mutate(r2 = ifelse(class == "all", 
   #                           regression_option1(d=build_final_data(lang), pr = pred),
    #                          regression_option1(build_final_data(lang), pr = pred))) 

```





```{r cross_validate_regression}

#divide into training and test set
#library(modelr)

#xvalr2 <- function(d, pr){
# n<-nrow(d) #df size
# ind <- sample(c(TRUE, FALSE), n, replace=TRUE, prob=c(0.9, 0.1)) #randomly split lines
# train <- d[ind, ] 
# test <- d[!ind, ]
# predictor <- train[[pr]]
# model <- lm(aoa~ predictor, data=train) 
# predictor <- test[[pr]]
# predictions <- predict(model, test)
# crossvalr2 <- rsquare(model, test)
# return(crossvalr2)
#}


#crossvalr2_ <- function(){
#  cross<- expand_grid(lang = c( "Italian"), 
#                  class = c("all", "nouns","adjectives","verbs", "function_words","other"),
#                  pred = c("freq", "mlu", "final_freq", "solo_freq", "init_freq", "char_number")) %>% 
#  rowwise %>%
#   mutate(crossvalr2 = ifelse(class == "all", 
#                             xvalr2(filter(d,   language == lang), pr = pred),
#                             xvalr2(filter(d,   language == lang, lexical_class == class), pr = pred))) #%>%                                              
#  rename( language = lang,lexical_class = class) 
#  return(cross)
#}

##crossvalr <- crossvalr2_() %>% 
##  rename(first = crossvalr2) %>%
##  full_join(crossvalr2_()) %>%
##  rename(second = crossvalr2) %>%
##  full_join(crossvalr2_()) %>%
##  mutate(crossval=first * second * crossvalr2) %>%
##  select(language, lexical_class, pred, crossval)

#crossval<- crossvalr2_() %>%
#  mutate(crossvalr= crossvalr2) %>%
#    select(language, lexical_class, pred, crossvalr)
  
#crossval %>%
#  knitr::kable(digits = 2)
```


```{r final_data}  
dfinal <- reliabilities %>%
  mutate(word_class = ifelse(word_class == "", 
                            "all", word_class)) %>%
  rename(language=lang, predictor=pred) %>%
  left_join(reliabilities_aoa) %>%
    mutate(threshold_half = sbr * split_half_aoa_sb) %>%
      left_join(unique(r2))# %>%
      #  select(-word_class) %>%
        #  filter(lexical_class=="all") %>%
       #    unique() %>%
        #  filter(!lexical_class=="other" )


dfinal %>%
  knitr::kable(digits = 2)
```


```{r final_plot_2}


ggplot(dfinal, aes(y = r2, x=predictor)) + 
   geom_bar(stat="identity") + 
  geom_point() + 
  facet_grid(cols = vars(word_class), rows = vars(language)) +
  geom_errorbar(data  = dfinal, aes(y=threshold_half, ymax=threshold_half, ymin=threshold_half)) + 
  theme(legend.position = "bottom") + 
  ylab("R2") + 
  xlab("Predictor") + 
  theme(legend.title = element_blank()) +
  theme(axis.text.x = element_text(angle = 70))

```



```{r reliability_alpha}

# cronbach_alpha <-function(dataAoa, corpus_frequency_){ 
#  lang_str <- normalize_language(lang) 
#   corpus <- read_csv(glue("data/childes/childes_tokens_{lang_str}.csv"))
#   corpus1 <- corpus %>% 
#     filter(gloss %in% wblemmas) %>% 
#       group_by(gloss, target_child_id) %>% 
#          summarize(count=n())  
#   corpus2 <- corpus %>%  
#     group_by(target_child_id) %>% 
#      summarize(total=n()) 
#   corpus <- corpus1 %>% 
#    left_join(corpus2) %>% 
#       mutate(freq=count/total) 

# corpus_ <- corpus %>% 
#        mutate(stem = stem(gloss, convert_stemlang(lang))) %>% 
#         full_join(load_unilemmas() %>% filter(language == normalize_language(lang)),
# by = "stem") %>% -->
#           group_by(uni_lemma, target_child_id) %>% 
#             mutate(FreqLemma = sum(freq, na.rm = TRUE))#

#   lemma_<-corpus_$uni_lemma  #restructure dataframe 
#   target_child_id_<-corpus_$target_child_id 
#   freq_<-corpus_$FreqLemma 

#   df<-unique(data.frame(lemma_, target_child_id_, freq_))
#   corpus<-tidyr::spread(df, target_child_id_, freq_) 

#  child_ids_<- unique(as.character(colnames(corpus)[3:ncol(corpus)])) 
#  child<-select(corpus, child_ids_ ) 
#   a<-alpha(child) 
#   return(a$total[1,1])  


```

### Measure intercept frequency

```{r intercept frequency}

#source("measure_frequency.R") # add intercept function to aoa-pipeline
#frequencies <- lapply(corpus, frequency_model) %>%bind_rows()

#frequencies %>% 
 # group_by(target_child_id) %>% 
  #summarize (sum(rawFrequency)) #Test frequence: should be 1 for each child

# TEST2 frequency metric:   
#frequencies %>% 
 # arrange(desc(FrequencyLog))#: maximum values

```

```{r final_plot_1}
#ggplot(d, aes(x = freq, y = aoa, col = lexical_class)) + 
#  geom_point(alpha = .1) + 
#  geom_smooth(method = "lm") + 
#  facet_grid(rows = vars(language)) +
#  facet_grid(language ~ lexical_class, scales = "free_x") + 
#  theme(legend.position = "bottom") + 
#  xlab("Frequency") + 
#  ylab("Age of production (months)")


#ggplot(d, aes(x = mlu, y = aoa, col = lexical_class)) + 
#  geom_point(alpha = .1) + 
#  geom_smooth(method = "lm") + 
#  facet_grid(rows = vars(language)) +
#  facet_grid(language ~ lexical_class, scales = "free_x") + 
#  theme(legend.position = "bottom") + 
#  xlab("MLU") + 
#  ylab("Age of production (months)")

#ggplot(d, aes(x = solo_freq, y = aoa, col = lexical_class)) + 
#  geom_point(alpha = .1) + 
#  geom_smooth(method = "lm") + 
#  facet_grid(rows = vars(language)) +
#  facet_grid(language ~ lexical_class, scales = "free_x") + 
#  theme(legend.position = "bottom") + 
#  xlab("Solo freq") + 
#  ylab("Age of production (months)")
```

