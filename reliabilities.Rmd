---
title: "Aoa_prediction_reliability_morphosyntax"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
options(dplyr.summarise.inform = FALSE)

library(tidyverse)
library(glue)
library(wordbankr)
#library(childesr)

normalize_language <- function(language) {
  language %>% str_replace(" ", "_") %>% str_to_lower()
}

walk(list.files("scripts", pattern = "*.R", full.names = TRUE), source)

#target_langs <- c("Russian", "Turkish", "Croatian", "Danish", "Italian")
target_langs <- c("Italian")

childes_path <- "data/childes/"

```


### Load saved CHILDES .csv corpus for a language (here French and Italian). 

Rbind different corpora.

```{r cdi}

load_cdi<- function(lang) {
  words <- get_inst_words(lang, "WS")
  admins <- get_inst_admins(lang, "WS")
  items <- get_instrument_data(language = lang,
                                 items = words$item_id,
                                 form = "WS",
                                 administrations = admins,
                                 iteminfo = words) %>%
                                 rename(lemma = definition) 
  return(items)
}
```

```{r aoas}
aoas <- map_df(target_langs, function(lang) {
  items <- load_cdi(lang)
  aoa <- fit_aoa(items, measure = "produces", method = "glmrob", proportion = 0.5)%>%
                                 filter(!is.na(aoa), !is.na(uni_lemma)) %>%
                                  mutate(language=lang)
  return(aoa)
})

```

```{r unilemmas}

wb_data <- map_df(target_langs, function(lang) {
  message(glue("Loading data for {lang}..."))
  norm_lang <- normalize_language(lang)
  readRDS(glue("data/wordbank/{norm_lang}.rds"))
})

uni_lemmas <- get_uni_lemmas(wb_data) %>%
  unnest(items) %>%
  left_join(aoas) %>%
  filter(!is.na(aoa), !is.na(uni_lemma)) %>%
  nest(items = c(category, definition)) %>%
  unique()
  
```

```{r predictors}

predictors <- map_dfr(target_langs, function(lang) {
  norm_lang <- normalize_language(lang)
  childes_lang <- convert_lang_childes(lang)
  totalcount <- nrow(readRDS(file.path(childes_path, glue("{childes_lang}_tokens.rds")))) ####TOFIX

  file_ <- file.path(childes_path, glue("uni_metrics_{norm_lang}.rds"))
  if(file.exists(file_)){  
  childes_predictors <- readRDS(file_)
  } else {
  get_childes_metrics(lang, metric_funs, corpus_args)
  childes_predictors <- get_uni_lemma_metrics(lang, uni_lemmas)
  }
  childes_predictors <- childes_predictors %>%
              group_by(language, uni_lemma) %>%
                summarise(tokens=tokens, 
                        mlu=mlu,
                        length_char=length_char,
                        freq = count/totalcount, 
                        solo_freq = count_solo/count,
                        final_freq = count_last/count,
                        init_freq = count_first/count)
}) 

```

### Plot frequency and aoa using log frequency and model intercept frequency
```{r plot}
#ggplot(d, 
#       aes(x=freq, y=aoa, label=uni_lemma)) + 
#    geom_smooth(method = "lm", formula = y~x) + 
#    geom_point(alpha=.1) +
#    facet_grid(lexical_class~language) +
#  scale_x_log10() + 
#  xlab("Frequency (log10)") + 
#  ylab("Age of Acquisition (months)") + 
#  theme_bw()

```
### Reliability_frequency: half-split and Spearman-Brown

```{r reliability}

# add lemmas of the first half not existing at the second half, with 1 
same_size_df <- function(df1, df2) { 
  firstlistlemma<-(df1$uni_lemma)
  secondlistlemma<-(df2$uni_lemma)
  diff1<-setdiff(firstlistlemma,secondlistlemma) 
  diff2<-setdiff(secondlistlemma,firstlistlemma) 
  df<-as.data.frame(c(diff1, diff2))
  colnames(df)<- c("uni_lemma")
  if (nrow(df> 0)){
    print("parts not equal")
    df1<- df1 %>%
      full_join(df) %>%
      arrange(uni_lemma)
    df2<- df2 %>%
      full_join(df)%>%
      arrange(uni_lemma)
  }else{}
  both=list(tibble(df1[!duplicated(df1$uni_lemma),]), tibble(df2[!duplicated(df2$uni_lemma),]))
  return(both)
}

######################
unilemma_half_cor<- function(half, lang){  # count raw frequency of half #word_class
  uni_lemma_map = build_uni_lemma_map(uni_lemmas)
  half_ <- half %>%
  mutate(stem = stem(token, convert_lang_stemmer(lang))) %>%
  inner_join(uni_lemma_map |> rename(token = option)) |>
  inner_join(uni_lemma_map |> rename(token_stem = option)) |>
  rename(tokens = token, token_stems = token_stem) |>
  group_by(uni_lemma) %>%
  mutate(language=lang) #%>%  
 # summarise(nb_forms=n()
  metrics_summaries <- list(
  half_ |> summarise(across(where(is_character), list)),
  half_ |> summarise(across(where(is_integer), sum)),
  half_ |> summarise(across(where(is_double), mean))
  )
  half_ <- metrics_summaries |>
    reduce(partial(left_join, by = "uni_lemma")) |>
  unique()
  half_ <- half_[!is.na(half_$count), ]
  return(half_[order(half_$uni_lemma),])
}

#######################  
default_metric_funs <- list(compute_count, compute_length_char)

split_half_cor <-function(lang){ #word_class
  childes_lang <- convert_lang_childes(lang)
  tokens <-readRDS(glue(childes_path, "{childes_lang}_tokens.rds"))
  utterances <- readRDS(glue(childes_path, "{childes_lang}_utterances.rds"))
  unnest_predictors <- predictors %>% filter(language == lang) %>% unnest(tokens)
  wblemmas<-unique(unnest_predictors$tokens) #used tokens
  tokens_shrank <- tokens %>%
    filter(gloss %in% wblemmas)
  ind <- sample(c(TRUE, FALSE), nrow(tokens_shrank), replace=TRUE, prob=c(0.5, 0.5)) #split tokens
  tokens1 <- tokens_shrank[ind, ] #split in two
  tokens2 <- tokens_shrank[!ind, ] #split in two
  split_df<- function(tokens){
     utterances <- utterances %>%
     filter(id %in% tokens$utterance_id)
     half <- list("utterances" = utterances, "tokens" = tokens)
     half<- get_childes_metrics(lang=lang, metric_funs = default_metric_funs, corpus_args, half=half)
    return(half)
  }
  both=list(tibble(split_df(tokens1)), tibble(split_df(tokens2)))
  return(both)  
  }
  

#######################
sbformula <- function(r){  #adjust with spearman-brown formula
  r1<-(2*r)/(1+r)
  return(r1)
}

#######################
fix_nas <- function(df){
 df<-df %>%
  mutate(char_number=ifelse(is.na(length_char), "0", length_char)) %>%
   mutate(freq=ifelse(is.na(count), "0", count)) %>%
return(df)
}
########################

main_split_half <- function(lang){
both<-split_half_cor(lang)
df1<-unilemma_half_cor(both[[1]], lang)
df2<-unilemma_half_cor(both[[2]], lang)                    
df<- same_size_df(df1,df2)
df[[1]]<-fix_nas(df[[1]])
df[[2]]<-fix_nas(df[[2]])
return(df)
}

corr_split<-function(lang){
df <- main_split_half(lang)  
length_char_r=cor(as.numeric(df[[1]]$length_char), as.numeric(df[[2]]$length_char), method="pearson", use="complete.obs")
count_r=cor(as.numeric(df[[1]]$count), as.numeric(df[[2]]$count), method="pearson", use="complete.obs")
r=c(length_char_r,count_r)
names= c("length_char","count")
rdata <- data.frame(names, r)
rdata <- rdata %>% mutate(language=lang)
return(rdata)
}

#########################

lang_pred_half <- function(lang, pred, split_half){
 s=split_half %>% filter(language==lang, names==pred)
 r=s[1,2]
return(as.character(r) ) 
}

``` 

### Reliability_frequency: cronbach alpha


### Measure reliabilities

```{r apply_reliability_frequency}

split_half <- lapply(target_langs, corr_split) %>%
  bind_rows() #####could not find croatianpy

reliabilities <- expand_grid(lang = c("Italian"),
                            word_class = c("all"),
                            pred = c("length_char", "count")) %>%  #"nouns","adjectives","verbs","function_words","other
                rowwise %>%            
 mutate(corrr =  lang_pred_half(lang, pred, split_half)) %>% #split_half
  mutate(sbr = sbformula(as.numeric(corrr)))

reliabilities %>%
  knitr::kable(digits = 2)


```  

### Reliability_AoA

```{r reliability_aoa}  

split_half_cor_aoa <-function(lang){
  #print(glue("Get CDI item data for {lang_} and {clas_} ..."))
  items <-load_cdi(lang)
  admin<-as.data.frame(unique(items$data_id))
  
  print(glue("Randomly split administrations..."))
  ind <- sample(c(TRUE, FALSE), nrow(admin), replace=TRUE, prob=c(0.5, 0.5)) #randomly split administrations
  adminfirstnum <- admin[ind, ]
  adminsecondnum <- admin[!ind, ] #create two groups of administrations
  
  adminfirst<-items %>% filter(data_id %in% adminfirstnum) #filter items in administrations
  adminsecond<-items %>% filter(data_id %in% adminsecondnum)
  
  aoafirst<- fit_aoa(adminfirst, method = "glmrob", proportion = 0.5) # get aoa for each group
  aoasecond<- fit_aoa(adminsecond, method = "glmrob", proportion = 0.5) # 
  
  r<-cor(aoafirst$aoa, aoasecond$aoa, use="complete.obs", method="pearson") #measure r
  return(r)
}


```

```{r apply_reliability_aoa}

reliabilities_aoa <- expand_grid(language = c( "Italian"),
                            word_class = c("all")) %>% # "nouns","adjectives","verbs",   "function_words","other"
  rowwise %>%
  mutate(split_half_aoa = ifelse(word_class == "all", 
                             split_half_cor_aoa(language),
                             split_half_cor_aoa(language, word_class)),
         split_half_aoa_sb = sbformula(split_half_aoa))

reliabilities_aoa %>%
  knitr::kable(digits = 2)


```

### Regression

```{r final_Data}
build_final_data <- function(lang){
predictor_data <- predictors %>% filter(language==lang)  %>%
    group_by(uni_lemma)
metrics_summaries <- list(
  predictor_data |> summarise(across(where(is_character), list)),
  predictor_data |> summarise(across(where(is_integer), sum)),
  predictor_data |> summarise(across(where(is_double), mean))
  )
  predictor_data <- metrics_summaries |>
    reduce(partial(left_join, by = "uni_lemma")) |>
  unique() %>%
  ungroup() %>%
  mutate(count=freq) %>% 
  select(uni_lemma, length_char, count)

aoas <- aoas %>% filter(language== lang)
 d <- aoas %>%
  mutate(language=lang) %>%  
  left_join(predictor_data) %>%
  ungroup() %>% 
  select(-c(item_id, num_item_id)) %>%
  filter(!is.na(aoa), !is.na(uni_lemma), !is.na(count))  %>%
  unique()

return(d)
}
```


```{r main_regression}

d <- lapply(target_langs, build_final_data) %>%
  bind_rows() #####could not find croatianpy




 regression_option1<-function(d,  pr){
 predictor <- d[[pr]]
 option1<-lm(aoa~predictor, d) 
 return(summary(option1)$adj.r.squared)
 }
# 
 r2 <- expand_grid(lang = c("Italian"), 
              #     class = c("all"),
                   pred = c("count",  "length_char")) %>% 
  rowwise %>%
  mutate(r2=regression_option1(d %>% filter(language==lang), pr = pred)) %>%
  rename( language = lang, predictor=pred)

```





```{r cross_validate_regression}

#divide into training and test set
#library(modelr)

#xvalr2 <- function(d, pr){
# n<-nrow(d) #df size
# ind <- sample(c(TRUE, FALSE), n, replace=TRUE, prob=c(0.9, 0.1)) #randomly split lines
# train <- d[ind, ] 
# test <- d[!ind, ]
# predictor <- train[[pr]]
# model <- lm(aoa~ predictor, data=train) 
# predictor <- test[[pr]]
# predictions <- predict(model, test)
# crossvalr2 <- rsquare(model, test)
# return(crossvalr2)
#}

```


```{r final_data}  
dfinal <- reliabilities %>%
  mutate(word_class = ifelse(word_class == "", 
                            "all", word_class)) %>%
  rename(language=lang, predictor=pred) %>%
  left_join(reliabilities_aoa) %>%
    mutate(threshold_half = sbr * split_half_aoa_sb) %>%
      left_join(unique(r2))

dfinal %>%
  knitr::kable(digits = 2)
```


```{r final_plot_2}


ggplot(dfinal, aes(y = r2, x=predictor)) + 
   geom_bar(stat="identity") + 
  geom_point() + 
  facet_grid(cols = vars(word_class), rows = vars(language)) +
  geom_errorbar(data  = dfinal, aes(y=threshold_half, ymax=threshold_half, ymin=threshold_half)) + 
  theme(legend.position = "bottom") + 
  ylab("R2") + 
  xlab("Predictor") + 
  theme(legend.title = element_blank()) +
  theme(axis.text.x = element_text(angle = 70))

```



```{r reliability_alpha}

# cronbach_alpha <-function(dataAoa, corpus_frequency_){ 
#  lang_str <- normalize_language(lang) 
#   corpus <- read_csv(glue("data/childes/childes_tokens_{lang_str}.csv"))
#   corpus1 <- corpus %>% 
#     filter(gloss %in% wblemmas) %>% 
#       group_by(gloss, target_child_id) %>% 
#          summarize(count=n())  
#   corpus2 <- corpus %>%  
#     group_by(target_child_id) %>% 
#      summarize(total=n()) 
#   corpus <- corpus1 %>% 
#    left_join(corpus2) %>% 
#       mutate(freq=count/total) 

# corpus_ <- corpus %>% 
#        mutate(stem = stem(gloss, convert_stemlang(lang))) %>% 
#         full_join(load_unilemmas() %>% filter(language == normalize_language(lang)),
# by = "stem") %>% -->
#           group_by(uni_lemma, target_child_id) %>% 
#             mutate(FreqLemma = sum(freq, na.rm = TRUE))#

#   lemma_<-corpus_$uni_lemma  #restructure dataframe 
#   target_child_id_<-corpus_$target_child_id 
#   freq_<-corpus_$FreqLemma 

#   df<-unique(data.frame(lemma_, target_child_id_, freq_))
#   corpus<-tidyr::spread(df, target_child_id_, freq_) 

#  child_ids_<- unique(as.character(colnames(corpus)[3:ncol(corpus)])) 
#  child<-select(corpus, child_ids_ ) 
#   a<-alpha(child) 
#   return(a$total[1,1])  


```

### Measure intercept frequency

```{r intercept frequency}

#source("measure_frequency.R") # add intercept function to aoa-pipeline
#frequencies <- lapply(corpus, frequency_model) %>%bind_rows()

#frequencies %>% 
 # group_by(target_child_id) %>% 
  #summarize (sum(rawFrequency)) #Test frequence: should be 1 for each child

# TEST2 frequency metric:   
#frequencies %>% 
 # arrange(desc(FrequencyLog))#: maximum values

```

```{r final_plot_1}
#ggplot(d, aes(x = freq, y = aoa, col = lexical_class)) + 
#  geom_point(alpha = .1) + 
#  geom_smooth(method = "lm") + 
#  facet_grid(rows = vars(language)) +
#  facet_grid(language ~ lexical_class, scales = "free_x") + 
#  theme(legend.position = "bottom") + 
#  xlab("Frequency") + 
#  ylab("Age of production (months)")


#ggplot(d, aes(x = mlu, y = aoa, col = lexical_class)) + 
#  geom_point(alpha = .1) + 
#  geom_smooth(method = "lm") + 
#  facet_grid(rows = vars(language)) +
#  facet_grid(language ~ lexical_class, scales = "free_x") + 
#  theme(legend.position = "bottom") + 
#  xlab("MLU") + 
#  ylab("Age of production (months)")

#ggplot(d, aes(x = solo_freq, y = aoa, col = lexical_class)) + 
#  geom_point(alpha = .1) + 
#  geom_smooth(method = "lm") + 
#  facet_grid(rows = vars(language)) +
#  facet_grid(language ~ lexical_class, scales = "free_x") + 
#  theme(legend.position = "bottom") + 
#  xlab("Solo freq") + 
#  ylab("Age of production (months)")
```

