---
title: "AoA prediction template"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
options(dplyr.summarise.inform = FALSE)
# load libraries
library(tidyverse)
library(modelr)
library(glue)
library(wordbankr)
library(childesr)
library(here)
# load functions
walk(list.files("scripts", pattern = "*.R$", full.names = TRUE), source)
```

This template provides the structure for how to fit age of acquisition (AoA) prediction models, using data and scripts in this repository.

The general steps are:
- loading the provided CDI data and predictor data
- adding your predictor(s) to the provided predictor data
- using the functions in `scripts/prep_data.R` to prepare the data for modeling
- using the functions in `scripts/fit_models.R` to fit models and extract information from them


# Load Wordbank data

Loading cached Wordbank data for English:
```{r load_wordbank_eng, eval=FALSE}
eng_wb_data <- load_wb_data("English (American)")
eng_wb_data
```

Defining languages:
```{r define_langs}
target_langs <- c("French (French)", "French (Quebecois)", 
                  "Italian",
                  "Spanish (Mexican)", "Spanish (European)",
                  "Portuguese (European)",  
                  "Dutch",
                  "English (American)", "English (Australian)", 
                  "English (British)", 
                  "German",
                  "Danish", "Norwegian", "Swedish", 
                  "Croatian", "Czech",
                  "Turkish", 
                  "Hungarian",
                  "Mandarin (Taiwanese)", "Mandarin (Beijing)",
                  "Russian",
                  "Hebrew")
```


Loading Wordbank data for multiple languages (cached or not):
```{r load_wordbank_xling}
wb_data <- load_wb_data(target_langs)
aoas <- fit_aoas(wb_data)
```

Creating saved Wordbank data for a language, for example:
```{r create_wordbank_eng, eval=FALSE}
eng_wb_data <- create_wb_data("English (American)")
```

Creating saved Wordbank data one step at a time (potentially making changes between steps), for example:
```{r create_wordbank_eng_steps, eval=FALSE}
eng_wg <- create_inst_data("English (American)", "WG")
eng_ws <- create_inst_data("English (American)", "WS")
eng_wg_summary <- collapse_inst_data(eng_wg)
eng_ws_summary <- collapse_inst_data(eng_ws)
eng_comb_summary <- combine_form_data(list(eng_wg_summary, eng_ws_summary))
```


# Load predictors

## Ratings and phonemes

Merge in the by-concept predictors (babiness, concreteness, etc) to the unilemmas and the by word predictors (phonemes) to the words/definitions.

```{r merge_unilemmas}
uni_lemmas <- map_df(target_langs, extract_uni_lemmas, wb_data)
```

```{r load_predictors}
babiness_map <- c(word = "word", babiness = "babyAVG")
babiness <- uni_lemmas |> map_predictor("babiness", babiness_map)
valence_map <- c(word = "Word", valence = "V.Mean.Sum", arousal = "A.Mean.Sum")
valence <- uni_lemmas |> map_predictor("valence", valence_map)
concreteness_map <- c(word = "Word", concreteness = "Conc.M")
concreteness <- uni_lemmas |> map_predictor("concreteness", concreteness_map)
```


## CHILDES

Get CHILDES data for many languages:
```{r get_childes, eval=FALSE}
corpus_args <- list(corpus = NULL, role = NULL, role_exclude = "Target_Child",
                    age = NULL, sex = NULL, part_of_speech = NULL, token = "*")
walk(target_langs, \(x) {get_childes_data(convert_lang_childes(x), corpus_args)})
```


Creating saved morphology metrics for many languages:
```{r create_morphs_xling, eval=FALSE}
walk(target_langs, load_morph_data, corpus_args)
```

Creating saved parsed data for many languages:
```{r parse_data, eval=FALSE}
walk(target_langs, get_parsed_data, corpus_args = corpus_args)
```


Loading cached CHILDES metrics for English:
```{r load_childes_eng, eval=FALSE}
eng_metrics <- load_childes_metrics("English (American)", uni_lemmas)
```

Creating saved CHILDES metrics for English, potentially changing which metrics are computed and/or arguments that are passed to `childesr` functions:
```{r specify_childes}
metric_funs <- list(base = list(compute_count, compute_mlu, compute_positions,
                                compute_length_char, compute_length_phon),
                    parsed = list(compute_form_entropy, compute_subcat_entropy,
                                  compute_n_features),
                    morph = list(compute_n_morphemes))
```


Creating saved CHILDES data for English:
```{r create_childes_english, eval=FALSE}
eng_childes <- get_token_metrics("English (American)", metric_funs, corpus_args)
eng_unilemmas <- get_uni_lemma_metrics("English (American)", build_uni_lemma_map(uni_lemmas))
childes_metrics = eng_unilemmas
```


Creating saved CHILDES data for many languages:
```{r create_childes_xling, eval=TRUE}
walk(target_langs, get_token_metrics, metric_funs, corpus_args)
```

Creating saved UNILEMMA data for many languages:
```{r create_unilemma_xling, eval=TRUE}
walk(target_langs, get_uni_lemma_metrics, build_uni_lemma_map(uni_lemmas))
```

Loading cached CHILDES data for multiple languages:
```{r load_unilemma_xling, eval=TRUE}
childes_metrics <- load_childes_metrics(target_langs, uni_lemmas) |> filter(!is.na(uni_lemma))
```


Get phonology via eSpeak for tokens that didn't get it from CHILDES:
```{r eval=FALSE}
phonemes <- uni_lemmas |> filter(uni_lemma != "NA") |> map_phonemes()
childes_metrics <- childes_metrics |>
  left_join(phonemes, by = c("language", "uni_lemma")) |>
  mutate(length_char = coalesce(length_char, num_chars),
         length_phon = coalesce(length_phon, num_phons)) |>
  select(-c(num_chars, num_phons))
```


# Prepare data for modeling

## Frequency transformations

By default, `transform_counts()` transforms any column that starts with "count" by smoothing (add 1), normalizing, and log transforming, then renaming every column "count_x" to "freq_x". `residualize_freqs()` residualizes all columns that starts with "freq_" from the column "freq".

```{r prepare_frequency}
childes_metrics <- childes_metrics |> transform_counts() |> residualize_freqs() 
```

```{r prepare_morphology}
childes_metrics <- map_df(target_langs, residualize_morph, childes_metrics)
```

## Combining sources

Combine mapped predictors and CHILDES predictors:

```{r merge_all}
predictor_data_list <- list(babiness, concreteness, childes_metrics)
predictor_data <- predictor_data_list |>
  reduce(partial(full_join, by = c("language", "uni_lemma"))) 
```

## Setting predictors

```{r set_predictors}
predictor_sources <- list(
  c("freq"),
  c("concreteness","babiness"),
  c("length_char", "length_phon"),
  c("n_features",  "form_entropy", "subcat_entropy"), 
  c("n_morphemes"),
  c("mlu"))
predictors <- unlist(predictor_sources)
```


## Preparing data for regression

```{r prep_data}
ref_cat ="nouns"
predictor_data_lexcat <- prep_lexcat(predictor_data, uni_lemmas, ref_cat) |>
  distinct()
```

```{r prep_data2}
max_steps = 20
predictor_data_imputed_scaled <- do_full_imputation(predictor_data_lexcat, predictor_sources, max_steps)
```

 
## Merge with AOAs

```{r merge_aoa}
aoa_predictor_data <- predictor_data_imputed_scaled |> 
  left_join(aoas) |>
  filter(aoa <= 36, 
         lexical_category != "NA",
         lexical_category != "unknown") |> 
  mutate(lexical_category = lexical_category |> 
           fct_drop() |> 
           fct_relevel(ref_cat, after = Inf) |> 
           `contrasts<-`(value = "contr.sum"))
```

## Run model 

```{r}
aoa_models <- fit_models(predictors, aoa_predictor_data,
                         lexcat_interactions = FALSE)

aoa_models_lexcat <- fit_models(predictors, aoa_predictor_data,
                                lexcat_interactions = TRUE)
```

## Model outputs

Coefficients:
```{r}
coefs <- aoa_models |> select(language, measure, coefs) |> unnest(coefs) 
```

Summary stats:
```{r}
sum_stats <- aoa_models |> select(language, measure, stats) |> unnest(stats)
```

Variance inflation factors:
```{r}
vifs <- aoa_models |> select(language, measure, vifs) |> unnest(vifs)
```


Everything above here has been verified. 














```{r testing}
# library(lmerTest)
morph_complexity <- read_csv("resources/cwals.csv") |> 
  select(iso_codes, n_feat, morph_complexity = cwals_new)

aoa_predictor_data <- aoa_predictor_data |> 
  nest(data = -language) |> 
  mutate(iso_codes = sapply(language, convert_lang_morph_complexity)) |> 
  left_join(morph_complexity) |> 
  unnest(data) |> 
  select(-iso_codes) |> 
  mutate(lexical_category = lexical_category |> 
           fct_drop() |> 
           fct_relevel(ref_cat, after = Inf) |> 
           `contrasts<-`(value = "contr.sum"))

coefs <- aoa_models |> select(language, measure, coefs) |> unnest(coefs) 
aoa_coefs <- aoa_models_lexcat |> select(language, measure, coefs) |> unnest(coefs) 

# ms_terms <- c("n_forms", "n_morph_categories", "n_affixes", 
#               "main_frame_prop", "mlu")

# swe, por, hun, ces missing morph info??

ms_terms <- c("Main frame prop", "MLU-w", "N forms", "N morph categories", "N affixes")

morph_coefs <- plt_lang_coefs |> filter(term %in% ms_terms) |> 
  nest(data = -language) |> 
  mutate(iso_codes = sapply(language, convert_lang_morph_complexity)) |> 
  left_join(morph_complexity) |> 
  unnest(data) |> 
  select(-iso_codes) |> 
  mutate(term = as.factor(term))

mc_mods <- map(c("production", "comprehension"), \(y) {
  map(ms_terms, \(x) {
    lm(estimate ~ morph_complexity, 
       data = morph_coefs |> filter(term == x, measure == y))
  })
})
  

# mc <- lme4::lmer(estimate ~ term * morph_complexity + (1|language), 
#                  data = morph_coefs)

library(broom.mixed)
# aoa_all_lang_model <- fit_all_lang_model(predictors[predictors != "length_char"], 
#                                          aoa_predictor_data,
#                                          lexcat_interactions = FALSE)
aoa_all_lang_model_2 <- fit_all_lang_model(predictors[predictors != "length_char"], 
                                         aoa_predictor_data,
                                         lexcat_interactions = FALSE)
aoa_all_lang_model_2b <- fit_all_lang_model(predictors[predictors != "length_char"], 
                                         aoa_predictor_data,
                                         lexcat_interactions = TRUE)
aoa_all_lang_model_3 <- fit_all_lang_model(predictors[predictors != "length_char"], 
                                         aoa_predictor_data,
                                         morphcomp_interactions = TRUE,
                                         lexcat_interactions = FALSE)
aoa_all_lang_model_3b <- fit_all_lang_model(predictors[predictors != "length_char"], 
                                         aoa_predictor_data,
                                         morphcomp_interactions = TRUE,
                                         lexcat_interactions = TRUE)
# coefs_comp <- aoa_all_lang_model |> select(measure, coefs) |> unnest(coefs) 
coefs_comp_2 <- aoa_all_lang_model_2 |> select(measure, coefs) |> unnest(coefs) 
coefs_comp_2b <- aoa_all_lang_model_2b |> select(measure, coefs) |> unnest(coefs) 
coefs_comp_3 <- aoa_all_lang_model_3 |> select(measure, coefs) |> unnest(coefs)
coefs_comp_3b <- aoa_all_lang_model_3b |> select(measure, coefs) |> unnest(coefs)
# coefs_comp <- aoa_complexity_models |> select(language, measure, coefs) |> unnest(coefs) 

predictors_old <- c("freq", "concreteness", "babiness", "length_phon", "mlu")
aoa_all_lang_model_old <- fit_all_lang_model(predictors_old, 
                                             aoa_predictor_data,
                                             lexcat_interactions = TRUE)
# aoa_all_lang_model_old <- fit_all_lang_model(predictors_old, 
#                                              aoa_predictor_data,
#                                              lexcat_interactions = TRUE)
coefs_comp_old <- aoa_all_lang_model_old |> select(measure, coefs) |> unnest(coefs)
```

```{r}
run_main_model<-function(lang, aoa_predictor_data, predictor_sources, 
                         uni_lemmas, ref_cat, lexcat_interactions = TRUE){
  print(lang)
  df = aoa_predictor_data |> filter(language == lang)
  
  pred_sources <- drop_predictors(predictor_sources, df)
  
  aoa_predictor_data <- aoa_predictor_data |> 
    select(-lexical_category) |>
    filter(language == lang)
  aoa_predictor_data_lexcat <- prep_lexcat(aoa_predictor_data, uni_lemmas |> 
                                             filter(language==lang), ref_cat) 
  ## Set again lexical category contrasts
  m <- fit_models(predictors, aoa_predictor_data_lexcat, lexcat_interactions) |> 
    mutate(language = lang)
  m
}
aoa_models <- map_df(target_langs, run_main_model, aoa_predictor_data, 
                     predictor_sources, uni_lemmas, ref_cat) |>
  unnest(coefs)
target_langs <- unique(aoa_models$language)  
#Some languages may not have enough resources to fit models, remove by target_langs
```

## Add per_frame feature only for verbs
```{r}
predictor_sources_verb <- list(
  c("freq"),
  c("concreteness","babiness"),
  c("length_char"), #"length_phon"
  c("n_type",  "n_cat", "n_affix"), 
  c("per_frame"),
  c("mlu"))
predictors_verb<-unlist(predictor_sources_verb)
aoa_models_verb <- map_df(target_langs, run_main_model, aoa_predictor_data|>filter(lexical_category =="predicates"), predictors_verb, uni_lemmas, ref_cat, lexcat_interactions = FALSE)
aoa_models_verb <- aoa_models_verb |> unnest(coefs) |>filter(term =="per_frame") 
common <- intersect(colnames(aoa_models), colnames(aoa_models_verb))
to_add <- aoa_models |>
  select(language, measure, group_data, stats, vifs)
#row-bind only on common column names
if (!(is.data.frame(aoa_models_verb) && nrow(aoa_models_verb)==0)){
  aoa_models <- rbind(aoa_models[common], aoa_models_verb[common])|>
    select(-c(group_data,stats, vifs )) |>
    nest(coefs =c(term, estimate, std.error, statistic, p.value)) |>
    left_join(to_add)
  } else {
  aoa_models <- aoa_models|>
  nest(coefs =c(term, estimate, std.error, statistic, p.value))
}  
```



## Cross-validation

```{r cross_validate}
run_cross_validation_model<-function(lang, meas, aoa_predictor_data, predictors, uni_lemmas, ref_cat){
  l = normalize_language(lang)
  aoa_predictor_data <- aoa_predictor_data|> select(-lexical_category)|>
    filter(language==lang, measure==meas)
  
  aoa_predictor_data_lexcat <- prep_lexcat(aoa_predictor_data, uni_lemmas |> filter(language==lang), ref_cat) ## Set again lexical category contrasts
  df = aoa_predictor_data|>filter(language ==lang)
  predictors <- drop_predictors(df)
loo_df <- aoa_predictor_data_lexcat |>
  group_nest(language, measure) |>
  mutate(language=lang,
         measure=meas,
    loo_models = ifelse(language %in% sinotibetan_languages, map(data, fit_cv_models, list(make_predictor_formula(predictors))), map(data, fit_cv_models, list(make_predictor_formula(predictors)))),
         loo_preds = map2(loo_models, data, get_cv_preds),
         cv_results = map(loo_preds, get_cv_results))  
return(loo_df)
}
```

```{r implement_xval}
# for simple cv result extraction
xval_und_model <- map_df(unique((aoa_predictor_data %>% filter(measure=="understands"))$language),run_cross_validation_model,"understands", aoa_predictor_data, predictors, uni_lemmas, ref_cat) 
xval_prod_model <- map_df(unique((aoa_predictor_data %>% filter(measure=="produces"))$language),run_cross_validation_model,"produces", aoa_predictor_data, predictors, uni_lemmas, ref_cat) 
loo_df <- bind_rows(xval_und_model, xval_prod_model)
```


```{r extract_cross_validated_results}
cv_results <- loo_df |>
  select(language, measure, cv_results) |>
  unnest(cv_results)
loo_preds <- loo_df |>
  select(language, measure, loo_preds, data)
cv_results_pos <- loo_preds |>
  unnest(loo_preds) |> 
  group_by(language, measure, lexical_category) |>
  summarise(mean_abs_dev = mean(abs_dev), sd_abs_dev = sd(abs_dev))
cv_results_cat <- loo_preds |>
  unnest(c(loo_preds, data), names_repair = ~ make.names(., unique = TRUE)) |>
  group_by(language, measure, category) |>
  summarise(mean_abs_dev = mean(abs_dev), sd_abs_dev = sd(abs_dev))
cv_results_lex <- loo_preds |>
  unnest(cols = c(loo_preds))|>
  group_by(language, measure, lexical_category) |>
  summarise(mean_abs_dev = mean(abs_dev), sd_abs_dev = sd(abs_dev))
eng_across_lang_lex_desc <- loo_preds |>
  unnest(cols = c(loo_preds))|>
  filter(language == "English (American)", measure == "produces") |>
  arrange(desc(abs_dev)) |>
  head(50)
man_t_across_lang_lex_desc <- loo_preds |>
  unnest(cols = c(loo_preds))|>
  filter(language == "Spanish (Mexican)", measure == "understands") |>
  arrange(desc(abs_dev)) |>
  head(50)
```


```{r extract_coefs}
lexcats <- set_names(
  paste0("lexical_category", 1:3),
  rownames(contrasts(aoa_models$group_data[[1]]$lexical_category))[1:3]
)
aoa_coefs <- aoa_models_lexcat |>
  select(language, measure, coefs) |>
  unnest(coefs) |>
  filter(term != "(Intercept)") |>
  mutate(signif = if_else(p.value < 0.05, TRUE, FALSE),
         effect = if_else(str_detect(term, ":"), "interaction", "main"),
         lexical_category = if_else(effect == "interaction",
                                    str_extract(term, "lexical_category[0-9]"),
                                    as.character(NA)),
         # lexical_category = fct_recode(lexical_category, !!!lexcats),
         term = if_else(effect == "interaction",
                        str_remove(term, ":?lexical_category[0-9]:?"),
                        term),
         # term = fct_recode(term, !!!lexcats),
         term = factor(term), #, levels = c(predictors, names(lexcats))
         language = factor(language, levels = target_langs))
```


```{r savedata, eval = TRUE }
saveRDS(predictor_data_lexcat,"data/plots/predictor_data_lexcat.rds")
saveRDS(aoa_predictor_data,"data/plots/aoa_predictor_data.rds" )
saveRDS(aoa_coefs, "data/plots/aoa_coefs.rds" )
saveRDS(cv_results, "data/plots/cv_results.rds" )
saveRDS(cv_results_pos, "data/plots/cv_results_pos.rds" )
saveRDS(cv_results_cat, "data/plots/cv_results_cat.rds" )
saveRDS(cv_results_lex, "data/plots/cv_results_lex.rds" )
saveRDS(eng_across_lang_lex_desc, "data/plots/eng_across_lang_lex_desc.rds" )
```


```{r save_worst_produced_data, eval = TRUE }
worst_predicted_unilemmas<- function(lang, meas, loo_preds){ loo_preds |>
  unnest(cols = c(loo_preds))|>
  filter(language == lang, measure == meas) |>
  arrange(desc(abs_dev)) |>
  head(50)
}
worst_uni_produce<-map_df(target_langs, worst_predicted_unilemmas, "produces", loo_preds) %>% 
  group_by(test_word) %>%
  summarise(count_lang=n(),
  lang_names = paste0(language, collapse = ","),
  aoa_mean=mean(aoa),
  aoa_pred_mean=mean(aoa_pred))
worst_uni_produce1<-loo_preds |>
  unnest(cols = c(loo_preds))|>
  filter(measure == "produces") |>
  filter(abs_dev>5)|>
  arrange(desc(abs_dev))
saveRDS(worst_uni_produce, "data/plots/worst_uni_produce.rds" )
```
