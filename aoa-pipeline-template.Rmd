---
title: "AoA prediction template"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
options(dplyr.summarise.inform = FALSE)

# load libraries
library(tidyverse)
library(modelr)
library(glue)
library(wordbankr)
library(childesr)

# load functions
walk(list.files("scripts", pattern = "*.R$", full.names = TRUE), source)
```

This template provides the structure for how to fit age of acquisition (AoA) prediction models, using data and scripts in this repository.

The general steps are:
- loading the provided CDI data and predictor data
- adding your predictor(s) to the provided predictor data
- using the functions in `scripts/prep_data.R` to prepare the data for modeling
- using the functions in `scripts/fit_models.R` to fit models and extract information from them


# Load Wordbank data

Loading cached Wordbank data for English:
```{r load_wordbank_eng}
eng_wb_data <- load_wb_data("English (American)")
eng_wb_data
```

Loading Wordbank data for multiple languages (cached or not):
```{r load_wordbank_xling}
target_langs <- c("Croatian", "Danish", "English (American)", "Norwegian",
                  "Russian", "Turkish", "Spanish (Mexican)", "Italian",
                  "Swedish",  "French (Quebecois)")
#"French (French)" "English (Australian)", "German","English (British)",
#Portuguese (European) Spanish (European)" have NA in unilemmas 

wb_data <- load_wb_data(target_langs)
```

Creating saved Wordbank data for a language, for example:
```{r create_wordbank_eng, eval=FALSE}
eng_wb_data <- create_wb_data("English (American)")
```

Creating saved Wordbank data one step at a time (potentially making changes between steps), for example:
```{r create_wordbank_eng_steps, eval=FALSE}
eng_wg <- create_inst_data("English (American)", "WG")
eng_ws <- create_inst_data("English (American)", "WS")
eng_wg_summary <- collapse_inst_data(eng_wg)
eng_ws_summary <- collapse_inst_data(eng_ws)
eng_comb_summary <- combine_form_data(list(eng_wg_summary, eng_ws_summary))
```


# Load predictors

## Ratings and phonemes

Merge in the by-concept predictors (babiness, concreteness, etc) to the unilemmas and the by word predictors (phonemes) to the words/definitions.

```{r merge_unilemmas}
uni_lemmas <- extract_uni_lemmas(wb_data)
```

```{r load_predictors}
babiness_map <- c(word = "word", babiness = "babyAVG")
babiness <- uni_lemmas |> map_predictor("babiness", babiness_map)

valence_map <- c(word = "Word", valence = "V.Mean.Sum", arousal = "A.Mean.Sum")
valence <- uni_lemmas |> map_predictor("valence", valence_map)

concreteness_map <- c(word = "Word", concreteness = "Conc.M")
concreteness <- uni_lemmas |> map_predictor("concreteness", concreteness_map)
```

## CHILDES

Loading cached CHILDES metrics for English:
```{r load_childes_eng}
eng_metrics <- load_childes_metrics("English (American)", uni_lemmas)
```

Loading cached CHILDES data for multiple languages:
```{r load_childes_xling}
childes_metrics <- load_childes_metrics(target_langs, uni_lemmas)
```

Creating saved CHILDES metrics for English, potentially changing which metrics are computed and/or arguments that are passed to `childesr` functions:
```{r specify_childes, eval=FALSE}
metric_funs <- list(compute_count, compute_mlu, compute_positions,
                    compute_length_char, compute_length_phon)
corpus_args <- list(corpus = NULL, role = NULL, role_exclude = "Target_Child",
                    age = NULL, sex = NULL, part_of_speech = NULL, token = "*")
eng_childes <- get_token_metrics("English (American)", metric_funs, corpus_args)

childes_metrics <- load_childes_metrics("English (American)", uni_lemmas)
```

Creating saved CHILDES data for many languages:
```{r create_childes_xling, eval=FALSE}
walk(target_langs, get_token_metrics, metric_funs, corpus_args)
walk(target_langs, load_childes_metrics, uni_lemmas)
```

```{r}
# TODO: get phonology for tokens that didn't get it from CHILDES
# http://espeak.sourceforge.net
# phonemes <- uni_lemmas |> map_phonemes()
# left_join(phonemes, by = c("language", "uni_lemma"))
```


# Prepare data for modeling

## Frequency transformations

By default, `transform_counts()` transforms any column that starts with "count" by smoothing (add 1), normalizing, and log transforming, then renaming every column "count_x" to "freq_x". `residualize_freqs()` residualizes all columns that starts with "freq_" from the column "freq".

```{r prepare_frequency}
childes_metrics <- childes_metrics |> transform_counts() |> residualize_freqs()
```

## Combining sources

Combine mapped predictors and CHILDES predictors:

```{r merge_all}
predictor_data_list <- list(babiness, valence, concreteness, childes_metrics)
predictor_data <- predictor_data_list |>
  reduce(partial(full_join, by = c("language", "uni_lemma")))
```

## Set lexical category contrasts

```{r}
predictor_data_lexcat <- prep_lexcat(predictor_data, uni_lemmas)
```

## Imputation

```{r impute_data}
predictor_sources <- list(
  c("freq", "freq_last", "freq_first", "freq_solo", "mlu"),
  c("arousal", "valence"),
  "concreteness",
  "babiness",
  "length_char",
  # "length_phon",
  "n_tokens"
)
predictors <- unlist(predictor_sources)

predictor_data_imputed <- predictor_data_lexcat |>
  do_full_imputation(predictor_sources, max_steps = 20)
```

## Scaling

```{r}
predictor_data_scaled <- do_scaling(predictor_data_imputed, predictors)
```

# Fit models

```{r aoa-lm}
aoas <- fit_aoas(wb_data)
aoa_predictor_data <- aoas |> left_join(predictor_data_scaled)

aoa_predictor_data <- aoa_predictor_data |>
  # russian sound data is messed up
  filter(!(language == "Russian" & category == "sounds"))

aoa_models <- fit_models(predictors, aoa_predictor_data)
```

## Model outputs

Coefficients:
```{r}
aoa_models |> select(language, measure, coefs) |> unnest(coefs)
```

Summary stats:
```{r}
aoa_models |> select(language, measure, stats) |> unnest(stats)
```

Variance inflation factors:
```{r}
aoa_models |> select(language, measure, vifs) |> unnest(vifs)
```


## Cross-validation

```{r cross_validate}
# for simple cv result extraction
loo_df <- aoa_predictor_data |>
  group_nest(language, measure) |>
  mutate(loo_models = map(data, fit_cv_models,
                          list(make_predictor_formula(predictors))),
         loo_preds = map2(loo_models, data, get_cv_preds),
         cv_results = map(loo_preds, get_cv_results))

cv_results <- loo_df |>
  select(language, measure, cv_results) |>
  unnest(cv_results)

############## plot by lex_category /abstract category and prediction error 
cv_results_pos <- loo_df |>
  select(language, measure, loo_preds) |>
  unnest(loo_preds) |> 
  group_by(language, measure, lexical_category) |>
  summarise(mean_abs_dev = mean(abs_dev), mean_se = mean(se))

cv_results_cat <- loo_df |>
  select(language, measure, loo_preds, data) |>
  unnest(c(loo_preds, data), names_repair = "unique") |>
  group_by(language, measure, category) |>
  summarise(mean_abs_dev = mean(abs_dev), mean_se = mean(se))

cv_results_lex <- loo_df |>
  select(c(language, measure, loo_preds)) |>
  unnest(cols = c(loo_preds))

# eng_across_lang_lex_desc <- cv_results_lex |>
#   filter(language == "English (American)", measure == "produces") |>
#   arrange(desc(abs_dev)) |>
#   head(50)
```

## Plot model results

```{r coefs}
lexcats <- set_names(
  paste0("lexical_category", 1:3),
  rownames(contrasts(aoa_models$group_data[[1]]$lexical_category))[1:3]
)
aoa_coefs <- aoa_models |>
  select(language, measure, coefs) |>
  unnest(coefs) |>
  filter(term != "(Intercept)") |>
  mutate(signif = if_else(p.value < 0.05, TRUE, FALSE),
         effect = if_else(str_detect(term, ":"), "interaction", "main"),
         lexical_category = if_else(effect == "interaction",
                                    str_extract(term, "lexical_category[0-9]"),
                                    as.character(NA)),
         lexical_category = fct_recode(lexical_category, !!!lexcats),
         term = if_else(effect == "interaction",
                        str_remove(term, ":?lexical_category[0-9]:?"),
                        term),
         term = fct_recode(term, !!!lexcats),
         term = factor(term, levels = c(predictors, names(lexcats))),
         language = factor(language, levels = target_langs))

# lang_coefs$term <- factor(lang_coefs$term, levels = c("frequency", "solo_frequency", "first_frequency", "final_frequency", "mlu", "length_char", "n_tokens", "babiness", "concreteness", "valence"))
# 
# lang_coefs$language <- factor(lang_coefs$language, levels=c("English (American)","English (British)", "French (Quebecois)", "Spanish (Mexican)","Swedish",  "Italian", "Croatian", "Danish",  "Norwegian",  "Turkish","Russian")) #"English (Australian)",   "German", "French (French)",
# coefs_by_lang(lang_coefs |> filter(language =="Russian", measure == "produces"))
# coefs_by_lang(lang_coefs |> filter(language == "English (American)"))
# coefs_by_lang(lang_coefs |> filter(!language == "Russian"))
```

```{r plots_langcoeff_all}
# all_coefs <- fitted_aoa_models |>
#   unnest(coefficients) |>
#   mutate(signif = if_else(p.value < 0.05, TRUE, FALSE))  
# 
# all_coefs <- all_coefs |>
#   mutate(interact_lexcat=ifelse(grepl("adverbs",term),'adverbs',term))|>
#   mutate(interact_lexcat=ifelse(grepl("verbs",term),'verbs',interact_lexcat))|>
#   mutate(interact_lexcat=ifelse(grepl("other",term),'other',interact_lexcat))|>
#   mutate(interact_lexcat=ifelse(grepl("adject",term),'adjectives',interact_lexcat))|>
#   mutate(interact_lexcat=ifelse(grepl("function",term),'function_words',interact_lexcat))|>
#   mutate(interact_lexcat=ifelse(grepl("^valence|^solo_frequency|^frequency|^concreteness|^length_char|^babiness|^n_tokens|^final_frequency|^first_frequency|^mlu|Intercept",term),'single',interact_lexcat))
# 
# coefs_by_pred_all(all_coefs |> filter(language !="Russian"))
```


```{r plots_crossval}
# cv_results |>
#   filter(!language == "English (British)", !language == "Russian") |>
#   cv_dev_lang()
# 
# dev_words(eng_across_lang_lex_desc)
# 
# u <- aoa_mad_lex_cat(eng_across_lang_lex_desc,
#                      "English (American)", "understands", wb_data, preds)  
# p <- aoa_mad_lex_cat(eng_across_lang_lex_desc,
#                      "English (American)", "produces", wb_data, preds)
# plot_grid(u, p)
# 
# cv_results_pos$mean_abs_dev <- as.numeric(cv_results_pos$mean_abs_dev) 
# ggplot(aes(x = mean_abs_dev, y = language),
#        data = cv_results_pos |> filter(!language == "Russian")) +
#   facet_wrap(~lexical_category, nrow=5) +
#   geom_point(aes(colour = factor(lexical_category)), size = 4)+
#   geom_errorbar(aes(xmin = mean_abs_dev - mean_se, xmax = mean_abs_dev + mean_se))+
#   geom_smooth(method = "loess") +
#   scale_color_brewer(palette = "Set1") +
#   theme_bw(base_size = 14) +
#   theme(panel.grid = element_blank())
# 
# cv_results_cat$mean_abs_dev <- as.numeric(cv_results_cat$mean_abs_dev) 
# ggplot(aes(x = mean_abs_dev, y = language),
#        data = cv_results_cat |> filter(!language=="Russian")) +
#   facet_wrap(~category) +
#   geom_point(aes(colour = factor(category)), size = 4)+
#   geom_errorbar(aes(xmin = mean_abs_dev - mean_se, xmax = mean_abs_dev + mean_se))+
#   geom_smooth(method = "loess") +
#   theme_bw(base_size = 14) +
#   theme(panel.grid = element_blank())
```
