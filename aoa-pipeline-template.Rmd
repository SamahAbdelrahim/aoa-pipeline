---
title: "AoA prediction template"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
options(dplyr.summarise.inform = FALSE)
# load libraries
library(tidyverse)
library(modelr)
library(glue)
library(wordbankr)
library(childesr)
library(here)
library(quanteda)
library(tmcn)
# load functions
walk(list.files("scripts", pattern = "*.R$", full.names = TRUE), source)
```

This template provides the structure for how to fit age of acquisition (AoA) prediction models, using data and scripts in this repository.

The general steps are:
- loading the provided CDI data and predictor data
- adding your predictor(s) to the provided predictor data
- using the functions in `scripts/prep_data.R` to prepare the data for modeling
- using the functions in `scripts/fit_models.R` to fit models and extract information from them


# Load Wordbank data

Loading cached Wordbank data for English:
```{r load_wordbank_eng, eval=FALSE}
eng_wb_data <- load_wb_data("English (American)")
eng_wb_data
```

Defining languages:
```{r define_langs}
target_langs <- c("French (French)", "French (Quebecois)", 
                  "Italian",
                  "Spanish (Mexican)", "Spanish (European)",
                  "Catalan",
                  "Portuguese (European)",  
                  "Dutch",
                  "English (American)", "English (Australian)", 
                  "English (British)", 
                  "German",
                  "Danish", "Norwegian", "Swedish", 
                  "Croatian", "Czech", "Russian",
                  "Turkish", 
                  "Hungarian", "Estonian",
                  "Mandarin (Taiwanese)", "Mandarin (Beijing)",
                  "Cantonese",
                  "Hebrew", # "Arabic (Saudi)",
                  "Korean",
                  "Japanese")
```


Loading Wordbank data for multiple languages (cached or not):
```{r load_wordbank_xling}
wb_data <- load_wb_data(target_langs, db_args = db_args)
aoas <- fit_aoas(wb_data)
```

Creating saved Wordbank data for a language, for example:
```{r create_wordbank_eng, eval=FALSE}
eng_wb_data <- create_wb_data("English (American)")
```

Creating saved Wordbank data one step at a time (potentially making changes between steps), for example:
```{r create_wordbank_eng_steps, eval=FALSE}
eng_wg <- create_inst_data("English (American)", "WG")
eng_ws <- create_inst_data("English (American)", "WS")
eng_wg_summary <- collapse_inst_data(eng_wg)
eng_ws_summary <- collapse_inst_data(eng_ws)
eng_comb_summary <- combine_form_data(list(eng_wg_summary, eng_ws_summary))
```


# Load predictors

## Ratings and phonemes

Merge in the by-concept predictors (babiness, concreteness, etc) to the unilemmas and the by word predictors (phonemes) to the words/definitions.

```{r merge_unilemmas}
uni_lemmas <- map_df(target_langs, extract_uni_lemmas, wb_data)
```

```{r load_predictors}
babiness_map <- c(word = "word", babiness = "babyAVG")
babiness <- uni_lemmas |> map_predictor("babiness", babiness_map)
valence_map <- c(word = "Word", valence = "V.Mean.Sum", arousal = "A.Mean.Sum")
valence <- uni_lemmas |> map_predictor("valence", valence_map)
concreteness_map <- c(word = "Word", concreteness = "Conc.M")
concreteness <- uni_lemmas |> map_predictor("concreteness", concreteness_map)
```


## CHILDES

Get CHILDES data for many languages:
```{r get_childes, eval=FALSE}
corpus_args <- list(corpus = NULL, role = NULL, role_exclude = "Target_Child",
                    age = NULL, sex = NULL, part_of_speech = NULL, token = "*")
```


Loading saved morphology metrics for many languages:
```{r create_morphs_xling, eval=FALSE}
walk(target_langs, load_morph_data, corpus_args)
```

Loading saved parsed data for many languages:
```{r parse_data, eval=FALSE}
walk(target_langs, load_parsed_data, corpus_args = corpus_args)
```


Loading cached CHILDES metrics for English:
```{r load_childes_eng, eval=FALSE}
eng_metrics <- load_childes_metrics("English (American)", uni_lemmas)
```

Creating saved CHILDES metrics for English, potentially changing which metrics are computed and/or arguments that are passed to `childesr` functions:
```{r specify_childes}
metric_funs <- list(base = list(compute_count, compute_cd, 
                                compute_mlu, compute_positions,
                                compute_length_char, compute_length_phon),
                    parsed = list(compute_form_entropy, compute_subcat_entropy,
                                  compute_n_features),
                    morph = list(compute_n_morphemes))
```


Creating saved CHILDES data for English:
```{r create_childes_english, eval=FALSE}
eng_childes <- get_token_metrics("English (American)", metric_funs, corpus_args)
eng_unilemmas <- get_uni_lemma_metrics("English (American)", build_uni_lemma_map(uni_lemmas))
childes_metrics = eng_unilemmas
```


Creating saved CHILDES data for many languages:
```{r create_childes_xling, eval=FALSE}
walk(target_langs, get_token_metrics, metric_funs, corpus_args)
```

Creating saved UNILEMMA data for many languages:
```{r create_unilemma_xling, eval=FALSE}
walk(target_langs, get_uni_lemma_metrics, build_uni_lemma_map(uni_lemmas))
```

Loading cached CHILDES data for multiple languages:
```{r load_unilemma_xling, eval=TRUE}
childes_metrics <- load_childes_metrics(target_langs, uni_lemmas) |> 
  filter(!is.na(uni_lemma), uni_lemma != "NA")
```


Get phonology via eSpeak for tokens that didn't get it from CHILDES:
```{r eval=TRUE}
phon_data <- uni_lemmas |> filter(uni_lemma != "NA") |> map_phonemes()
phon_metrics <- phon_data |> compute_phon_metrics()
childes_metrics <- childes_metrics |>
  left_join(phon_metrics, by = c("language", "uni_lemma")) |>
  mutate(length_char = coalesce(length_char, num_chars),
         length_phon = coalesce(length_phon, num_phons)) |>
  select(-c(num_chars, num_phons))
```


# Prepare data for modeling

## Frequency transformations

By default, `transform_counts()` transforms any column that starts with "count" by smoothing (add 1), normalizing, and log transforming, then renaming every column "count_x" to "freq_x". `residualize_freqs()` residualizes all columns that starts with "freq_" from the column "freq".

```{r prepare_frequency}
childes_metrics <- childes_metrics |> 
  nest(data = -language) |> 
  mutate(data = map(data, transform_counts),
         data = map(data, residualize_freqs))
```

```{r residualize}
semantic_preds <- c("concreteness", "babiness")
distrib_preds <- c("freq", "cd")
phon_preds <- c("length_phon", "phon_neighbors")
morph_preds <- c("form_entropy", "n_features", "n_morphemes")
syntax_preds <- c("mlu", "subcat_entropy")

for (preds in list(distrib_preds, phon_preds, morph_preds, syntax_preds)) {
  childes_metrics <- childes_metrics |> 
    mutate(data = map(data, \(d) {
      d |> mutate(across(all_of(preds[-1]), 
                         partial(residualize_col, 
                                 residualizing_column = d |> pull(preds[1]))))}))
}
childes_metrics <- childes_metrics |> unnest(data)
```


## Combining sources

Combine mapped predictors and CHILDES predictors:

```{r merge_all}
predictor_data_list <- list(babiness, concreteness, childes_metrics)
predictor_data <- predictor_data_list |>
  reduce(partial(full_join, by = c("language", "uni_lemma"))) 
```

## Setting predictors

```{r set_predictors}
predictor_sources <- list(
  c("freq"), # "cd"
  c("concreteness", "babiness"),
  c("length_phon", "phon_neighbors"),
  c("n_features", "form_entropy", "subcat_entropy"), 
  c("n_morphemes"),
  c("mlu"))
predictors <- unlist(predictor_sources)
```


## Preparing data for regression

```{r prep_data}
ref_cat = "nouns"
predictor_data_lexcat <- prep_lexcat(predictor_data, uni_lemmas, ref_cat) |>
  distinct()
```

```{r prep_data2}
max_steps = 20
predictor_data_imputed_scaled <- do_full_imputation(predictor_data_lexcat, predictor_sources, max_steps)
```

```{r}
# specific handling for subcat_entropy for non-verbs
# predictor_data_imputed_scaled <- predictor_data_imputed_scaled |> 
#   mutate(subcat_entropy = ifelse(lexical_category == "verbs", subcat_entropy, 0))
```

## Merge with AOAs

```{r merge_aoa}
aoa_predictor_data <- predictor_data_imputed_scaled |> 
  left_join(aoas, by = c("language", "uni_lemma"), relationship = "many-to-many") |>
  filter(aoa <= 36, 
         lexical_category != "NA",
         lexical_category != "unknown") |> 
  mutate(lexical_category = lexical_category |> 
           fct_drop() |> 
           fct_relevel(ref_cat, after = Inf) |> 
           `contrasts<-`(value = "contr.sum"))
```

# Modelling

## Run model 

```{rm eval=F}
aoa_models <- fit_models(predictors, aoa_predictor_data,
                         lexcat_interactions = FALSE)

aoa_models_lexcat <- fit_models(predictors, aoa_predictor_data,
                                lexcat_interactions = TRUE)
```

```{r}
aoa_models <- readRDS("aoa_models.rds") |> filter(language != "Arabic (Saudi)")
aoa_models_lexcat <- readRDS("aoa_models_lexcat.rds") |> filter(language != "Arabic (Saudi)")
```


## Model outputs

Coefficients:
```{r}
coefs <- aoa_models |> select(language, measure, coefs) |> unnest(coefs) 
```

```{r extract_coefs}
lexcats <- set_names(
  paste0("lexical_category", 1:2),
  rownames(contrasts(aoa_models_lexcat$group_data[[1]]$lexical_category))[1:2]
)

aoa_coefs <- aoa_models_lexcat |>
  select(language, measure, coefs) |>
  unnest(coefs) |>
  filter(term != "(Intercept)") |>
  mutate(# signif = if_else(p.value < 0.05, TRUE, FALSE),
         effect = if_else(str_detect(term, ":"), "interaction", "main"),
         lexical_category = str_extract(term, "lexical_category[0-9]"),
         # lexical_category = fct_recode(lexical_category, !!!lexcats),
         term = if_else(effect == "interaction",
                        str_remove(term, ":?lexical_category[0-9]:?"),
                        term),
         # term = fct_recode(term, !!!lexcats),
         term = factor(term), #, levels = c(predictors, names(lexcats))
         language = factor(language, levels = target_langs),
         term_cat = fct_collapse(term,
                                 Morphological = c("N features", "Form entropy", "N morphemes"),
                                 Syntactic = c("MLU-w", "Subcat entropy"),
                                 Phonological = c("Length in phonemes", "Phon neighbors"),
                                 Other = c("Frequency", "Contextual diversity", 
                                           "Concreteness", "Babiness")) |> 
                  fct_rev() |> 
                  fct_shift(-2))
                

n_by_lexcat <- aoa_predictor_data |> 
  group_by(language, lexical_category) |> 
  summarise(n = n())

n_by_lang <- n_by_lexcat |> 
  group_by(language) |> 
  summarise(n = sum(n))
```

```{r}
display_predictors <- function(predictors) {
  predictors |> 
    str_replace_all("_", " ") |> 
    str_to_sentence() |>
    str_replace("Mlu", "MLU-w") |> 
    str_replace("Length phon", "Length in phonemes") |> 
    str_replace("Freq", "Frequency") |> 
    str_replace("Cd", "Contextual diversity")
}

term_fct <- c("Frequency", # "Contextual diversity", 
              "Concreteness", "Babiness", 
              "Length in phonemes", "Phon neighbors",
              "N features", "Form entropy", "N morphemes",
              "Subcat entropy", "MLU-w"
              )

plot_coefs <- aoa_coefs |> 
  filter(is.na(lexical_category)) |> 
  mutate(term = term |> 
           fct_relabel(display_predictors) |> 
           fct_relevel(term_fct) |> 
           fct_rev(),
         measure = fct_recode(measure, 
                              "comprehension" = "understands",
                              "production" = "produces"),
         #signif = ifelse(p.value < .05, "significant", "non-significant") |> 
         #  fct_rev()
         ) |> 
  filter(!is.na(term)) |> 
  left_join(n_by_lang, by = "language") |> 
  mutate(signif = ifelse(conf.low < 0 & conf.high > 0,
                         "Not significant", "Significant"))

lc_plot <- aoa_coefs |> 
  select(language, measure, term, estimate, lexical_category) |> 
  pivot_wider(names_from = lexical_category, values_from = estimate) |> 
  mutate(predicates = `NA` + lexical_category1,
         function_words = `NA` + lexical_category2,
         nouns = `NA` - lexical_category1 - lexical_category2) |> 
  select(-c(`NA`, lexical_category1, lexical_category2)) |> 
  pivot_longer(cols = c(predicates, function_words, nouns),
               names_to = "lexical_category",
               values_to = "estimate") |> 
  filter(!is.na(estimate)) |> 
  mutate(term = term |> 
           fct_relabel(display_predictors) |> 
           fct_relevel(term_fct) |> 
           fct_rev(),
         measure = fct_recode(measure, 
                              "comprehension" = "understands",
                              "production" = "produces"),
         # signif = ifelse(p.value < .05, "significant", "non-significant")
         ) |> 
  filter(!is.na(term)) |> 
  left_join(n_by_lexcat, by = c("language", "lexical_category"))
```


Summary stats:
```{r}
sum_stats <- aoa_models_lexcat |> select(language, measure, stats) |> unnest(stats)
```

Variance inflation factors:
```{r}
vifs <- aoa_models |> select(language, measure, vifs) |> unnest(vifs)
```


## Cross-validation (temporarily eval=F)

```{r, eval=F}
baseline_preds <- c("freq", "concreteness", "babiness")
phon_preds <- c("length_phon", "phon_neighbors")
morph_preds <- c("form_entropy", "n_features", "n_morphemes")
syntax_preds <- c("mlu", "subcat_entropy")

pred_lists <- list(baseline_preds,
                   c(baseline_preds, phon_preds),
                   c(baseline_preds, morph_preds),
                   c(baseline_preds, syntax_preds),
                   c(baseline_preds, phon_preds, morph_preds),
                   c(baseline_preds, phon_preds, syntax_preds),
                   c(baseline_preds, morph_preds, syntax_preds),
                   c(baseline_preds, phon_preds, morph_preds, syntax_preds))

# takes ~40min
# contrasts dropped from lexical_category: some langs only have n_morphemes for predicates!?
loo_data <- aoa_predictor_data |> 
  nest(data = -c(language, measure)) |> 
  mutate(loo_models = map(data, fit_cv_models, pred_lists),
         loo_preds = map2(loo_models, data, get_cv_preds),
         cv_results = map(loo_preds, get_cv_results))
```

```{r extract_cross_validated_results, eval=F}
cv_results <- loo_data |>
  select(language, measure, cv_results) |>
  unnest(cv_results) |> 
  mutate(name = name |> factor() |> 
           fct_recode(baseline = "1", 
                      phon = "2", morph = "3", syntax = "4",
                      phon_morph = "5", phon_syntax = "6", morph_syntax = "7",
                      full = "8"))
```



```{r morph_complexity}
# Exploratory
library(broom)
morph_complexity <- read_csv("resources/cwals.csv") |> 
  select(iso_codes, n_feat, morph_complexity = cwals_new)

ms_terms <- c("N features", "Form entropy", "N morphemes", "MLU-w") # "Subcat entropy", 

# note: uses lc_plot, which is from plots.Rmd
morph_coefs <- lc_plot |> 
  filter(term %in% ms_terms) |> 
  nest(data = -language) |> 
  mutate(iso_codes = sapply(language, convert_lang_morph_complexity)) |> 
  left_join(morph_complexity) |> 
  unnest(data) |> 
  select(-iso_codes) |> 
  mutate(term = as.factor(term))

lexcats <- c("nouns", "predicates", "function_words")

mc_mods <- map(c("production", "comprehension"), \(y) {
  map(ms_terms, \(x) {
    map(lexcats, \(lc) {
      lm(estimate ~ morph_complexity, 
       data = morph_coefs |> 
         filter(term == x, measure == y, lexical_category == lc))
    })
  }) |> unlist(recursive = FALSE)
}) |> unlist(recursive = FALSE)

mc_vals <- expand_grid(measure = c("production", "comprehension"),
                       ms_term = ms_terms,
                       lexical_category = lexcats) |> 
  mutate(mc_data = map(mc_mods, tidy)) |> 
  unnest(mc_data) |> 
  mutate(signif = p.value < .05)
```


Everything above here has been verified. 


Temporary: plots
```{r}
theme_mikabr <- function(base_size = 14, base_family = "Open Sans") {
  ggplot2::`%+replace%`(
    ggplot2::theme_bw(base_size = base_size, base_family = base_family),
    ggplot2::theme(panel.grid = ggplot2::element_blank(),
                   strip.background = ggplot2::element_blank(),
                   legend.key = ggplot2::element_blank())
  )
}
```


```{r}
ggplot(plot_coefs |> filter(!is.na(estimate), !is.na(term), term != "(Intercept)", is.na(group)) |> 
         mutate(language = factor(language, levels = target_langs),
                term_cat = fct_collapse(term,
                                        Morphological = c("N features", "Form entropy", "N morphemes"),
                                        Syntactic = c("MLU-w", "Subcat entropy"),
                                        Phonological = c("Length in phonemes", "Phon neighbors"),
                                        Other = c("Frequency", "Contextual diversity", 
                                                  "Concreteness", "Babiness")) |> 
                  fct_rev() |> 
                  fct_shift(-2)) |> 
         filter(measure == "production"), 
       aes(x = estimate, y = term)) +
  facet_grid(term_cat ~ ., scales = "free", space = "free") +
  geom_point(aes(shape = signif, color = language, size = n),  alpha = 0.7) +
  scale_shape_manual(values = c(21, 16)) +
  stat_summary(geom = "crossbar", fun = median, fun.min = median,
               fun.max = median, fatten = 2, width = 0.5) +
  geom_vline(xintercept = 0, color = "grey", linetype = "dotted") +
  labs(x = "Coefficient estimate", y = "", 
       shape = "Significance", color = "Language")+
  theme(legend.position="none") +
  theme_mikabr() +
  coord_cartesian(xlim = c(-2.5, 2.5))
```

```{r}
label_caps <- function(value) {
  value |> 
    str_to_sentence() |> 
    str_replace_all("_", " ")
}

ggplot(lc_plot |> 
         mutate(language = factor(language, levels = target_langs),
                lexical_category = factor(lexical_category, levels = 
                                            c("nouns", "predicates", "function_words")) |> 
                  fct_relabel(label_caps),
                term_cat = fct_collapse(term,
                                        Morphological = c("N features", "Form entropy", "N morphemes"),
                                        Syntactic = c("MLU-w", "Subcat entropy"),
                                        Phonological = c("Length in phonemes", "Phon neighbors"),
                                        Other = c("Frequency", "Contextual diversity", 
                                                  "Concreteness", "Babiness")) |> 
                  fct_rev() |> 
                  fct_shift(-2),
                term = term |> fct_relevel("Babiness", "Concreteness", "Frequency")) |> 
         filter(measure == "production"), 
       aes(x = estimate, y = term)) +
  facet_grid(term_cat ~ lexical_category, scales = "free", space = "free") +
  geom_point(aes(color = language, size = n),  alpha = 0.7) +
  # scale_shape_manual(values = c(16, 21)) +
  stat_summary(geom = "crossbar", fun = median, fun.min = median,
               fun.max = median, fatten = 2, width = 0.5) +
  geom_vline(xintercept = 0, color = "grey", linetype = "dotted") +
  labs(x = "Coefficient estimate", y = "") +
  theme_mikabr() +
  theme(legend.position = "none") +
  coord_cartesian(xlim = c(-3.5, 3.5))
```

```{r correlo-dendrogram}
dd_coefs <- aoa_coefs |> 
  filter(# !str_detect(term, "lexical_category"),
         is.na(lexical_category), 
         term != "sd__Observation") |> 
  select(language, measure, term, lexical_category, estimate) |> 
  filter(measure == "produces") |> 
  pivot_wider(names_from = c(term, lexical_category),
              values_from = estimate) |> 
  select(-measure)

dd_matrix <- dd_coefs |> 
  select(-language) |> 
  as.matrix() |> 
  `rownames<-`(dd_coefs$language)

COR_SCALE <- colorRampPalette(c("#BB4444", "#EE9988", "#FFFFFF", "#77AADD", "#4477AA"))

GERMANIC <- "#cc0000"
ROMANCE <- "#e69138"
SLAVIC <- "#f1c232"
JAPONIC <- "#8db63d"
KOREANIC <- "#47ad44"
SEMITIC <- "#45816e"
SINOTIBETAN <- "#3d85c6"
TURKIC <- "#3d50c6"
URALIC <- "#674ea7"

lang_cols <- c(# SEMITIC, 
               SINOTIBETAN, ROMANCE, SLAVIC,
               SLAVIC, GERMANIC, GERMANIC, GERMANIC,
               GERMANIC, GERMANIC, URALIC, ROMANCE, 
               ROMANCE, GERMANIC, SEMITIC, URALIC, 
               ROMANCE, KOREANIC, SINOTIBETAN, 
               SINOTIBETAN, GERMANIC, ROMANCE, SLAVIC, 
               ROMANCE, ROMANCE, GERMANIC, TURKIC, JAPONIC)

dd_heatmap <- gplots::heatmap.2(cor(t(dd_matrix), use = "pairwise.complete.obs"),
                        dendrogram = "column",
                        trace = "none",
                        col = COR_SCALE(30),
                        colRow = lang_cols,
                        colCol = lang_cols,
                        mar = c(10, 10))
```





```{r}
cv_plot <- cv_results |> 
  filter(measure == "produces") |> 
  select(language, name, mean_abs_dev, ci_mad) |> 
  pivot_wider(names_from = name,
              values_from = c(mean_abs_dev, ci_mad)) |> 
  mutate(across(starts_with("mean_abs_dev_"), \(v) {v - mean_abs_dev_baseline})) |> 
  pivot_longer(cols = -language,
               names_to = c(".value", "name"),
               names_pattern = "(mean_abs_dev|ci_mad)_(.*)") |> 
  mutate(Model = name |> 
           fct_relevel("phon", "morph", "syntax",
                       "phon_morph", "phon_syntax", "morph_syntax",
                       "full") |> 
           fct_relabel(\(v) str_replace_all(v, "_", "+")))

ggplot(cv_plot |> filter(name != "baseline"), 
       aes(x = mean_abs_dev, y = language, col = Model)) +
  geom_point() +
  geom_vline(xintercept = 0, color = "grey", lty = "dotted") +
  labs(x = "Mean absolute deviation (mo)", y = "Language") +
  theme_mikabr()
```


```{r}
mlu_pred <- morph_coefs |> 
  filter(measure == "production",
         term == "MLU-w",
         lexical_category == "predicates")
ggplot(mlu_pred, aes(x = morph_complexity, y = estimate)) +
  geom_point(aes(col = language)) + 
  geom_smooth(method = "lm", col = "black") +
  geom_hline(yintercept = 0, color = "grey", lty = "dotted") +
  geom_label_repel(aes(label = language, col = language), size = 2) +
  labs(x = "Estimated morphological complexity", 
       y = "Coefficient estimate",
       col = "Language") +
  theme_mikabr() +
  theme(legend.position = "none")
```











```{r all_lang_model}
library(broom.mixed)
# aoa_all_lang_model <- fit_all_lang_model(predictors[predictors != "length_char"], 
#                                          aoa_predictor_data,
#                                          lexcat_interactions = FALSE)
aoa_all_lang_model_2 <- fit_all_lang_model(predictors[predictors != "length_char"], 
                                         aoa_predictor_data,
                                         lexcat_interactions = FALSE)
aoa_all_lang_model_2b <- fit_all_lang_model(predictors[predictors != "length_char"], 
                                         aoa_predictor_data,
                                         lexcat_interactions = TRUE)
aoa_all_lang_model_3 <- fit_all_lang_model(predictors[predictors != "length_char"], 
                                         aoa_predictor_data,
                                         morphcomp_interactions = TRUE,
                                         lexcat_interactions = FALSE)
aoa_all_lang_model_3b <- fit_all_lang_model(predictors[predictors != "length_char"], 
                                         aoa_predictor_data,
                                         morphcomp_interactions = TRUE,
                                         lexcat_interactions = TRUE)
# coefs_comp <- aoa_all_lang_model |> select(measure, coefs) |> unnest(coefs) 
coefs_comp_2 <- aoa_all_lang_model_2 |> select(measure, coefs) |> unnest(coefs) 
coefs_comp_2b <- aoa_all_lang_model_2b |> select(measure, coefs) |> unnest(coefs) 
coefs_comp_3 <- aoa_all_lang_model_3 |> select(measure, coefs) |> unnest(coefs)
coefs_comp_3b <- aoa_all_lang_model_3b |> select(measure, coefs) |> unnest(coefs)
# coefs_comp <- aoa_complexity_models |> select(language, measure, coefs) |> unnest(coefs) 

predictors_old <- c("freq", "concreteness", "babiness", "length_phon", "mlu")
aoa_all_lang_model_old <- fit_all_lang_model(predictors_old, 
                                             aoa_predictor_data,
                                             lexcat_interactions = TRUE)
# aoa_all_lang_model_old <- fit_all_lang_model(predictors_old, 
#                                              aoa_predictor_data,
#                                              lexcat_interactions = TRUE)
coefs_comp_old <- aoa_all_lang_model_old |> select(measure, coefs) |> unnest(coefs)
```



```{r save_worst_produced_data, eval = FALSE}
worst_predicted_unilemmas<- function(lang, meas, loo_preds){ loo_preds |>
  unnest(cols = c(loo_preds))|>
  filter(language == lang, measure == meas) |>
  arrange(desc(abs_dev)) |>
  head(50)
}
worst_uni_produce<-map_df(target_langs, worst_predicted_unilemmas, "produces", loo_preds) %>% 
  group_by(test_word) %>%
  summarise(count_lang=n(),
  lang_names = paste0(language, collapse = ","),
  aoa_mean=mean(aoa),
  aoa_pred_mean=mean(aoa_pred))
worst_uni_produce1<-loo_preds |>
  unnest(cols = c(loo_preds))|>
  filter(measure == "produces") |>
  filter(abs_dev>5)|>
  arrange(desc(abs_dev))
saveRDS(worst_uni_produce, "data/plots/worst_uni_produce.rds" )
```
