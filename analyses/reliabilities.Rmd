---
title: "Aoa_prediction_reliability"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
options(dplyr.summarise.inform = FALSE)

library(tidyverse)
library(glue)
library(wordbankr)
library(broom)
library(childesr)
library(modelr)

walk(list.files("scripts", pattern = "*.R", full.names = TRUE), source)

# ok languages croatian, danish, english (american), nowegian, italian spanish turkish, swedish, french (quebecois), russian
target_langs <- c("Italian" )

predictor_list <- c("length_char","n_tokens", "mlu", "frequency", "solo_frequency", "first_frequency", "final_frequency")

childes_path <- "data/childes"

metric_funs <- list(compute_count, compute_mlu, compute_positions,
                    compute_length_char, compute_length_phon)
corpus_args <- list(corpus = NULL, role = NULL, role_exclude = "Target_Child",
                    age = NULL, sex = NULL, part_of_speech = NULL, token = "*")

```


### Functions

```{r cdi}

#get wordbank items and unilemmas, extract predictors for unilemmas in childes
get_predictors<- function(half, lang){
     token_metrics<- get_childes_metrics(lang=lang, metric_funs, corpus_args, import_data=half)
     wg <- tryCatch({create_inst_data(lang, "WG")}, error = function(e){})
     ws <- tryCatch({create_inst_data(lang, "WS")}, error = function(e){})
     wg_summary <- tryCatch({collapse_inst_data(wg)}, error = function(e){})
     ws_summary <- tryCatch({collapse_inst_data(ws)}, error = function(e){})
     if (!is.null(wg) & !is.null(ws)){
      comb_summary <- combine_form_data(list(wg_summary, ws_summary))
     } else if (!is.null(wg)){
      comb_summary <- wg_summary 
     } else if (!is.null(ws)){
      comb_summary <- ws_summary 
     } else{
      comb_summary <- NULL 
     }
     if (!is.null(comb_summary)){
      uni_lemmas <- get_uni_lemmas(comb_summary)
      uni_metrics<- get_uni_lemma_metrics(lang, build_uni_lemma_map(uni_lemmas), token_metrics)
      uni_metrics <- prepare_frequency(lang, uni_metrics, uni_lemmas, count, count_first, count_last, count_solo, frequency, first_frequency, final_frequency, solo_frequency) 
     return(uni_metrics[order(uni_metrics$uni_lemma),])
     } else{
      uni_metrics <- NA  
     return(uni_metrics)   
     }
}
#############################

#get childes data and randomly splits in half
split_half <-function(lang){ 
  childes_lang <- convert_lang_childes(lang)
  file_t <- file.path(childes_path, glue("{childes_lang}_tokens.rds"))
  if(file.exists(file_t)) {
   tokens <-readRDS(file_t)
  }else{
   tokens <- get_childes_data(childes_lang, corpus_args)[[2]] 
  }
  tokens <- tokens %>% mutate(gloss=tolower(gloss))
  file_u <- file.path(childes_path, glue("{childes_lang}_utterances.rds"))
  if(file.exists(file_u)) {
   utterances <- readRDS(file_u)
  }else{
   utterances <- get_childes_data(childes_lang, corpus_args)[[1]] 
  } 
 
  ind <- sample(c(TRUE, FALSE), nrow(tokens), replace=TRUE, prob=c(0.5, 0.5)) #split tokens
  tokens1 <- tokens[ind, ] #split in two
  tokens2 <- tokens[!ind, ] 
  
   utterances1 <- utterances %>%
     filter(id %in% tokens1$utterance_id) %>%
     mutate(gloss=tolower(gloss))  

    utterances2 <- utterances %>%
     filter(id %in% tokens2$utterance_id) %>%
     mutate(gloss=tolower(gloss)) 
    
  half1 <- list("utterances" = utterances1, "tokens" = tokens1)
  half2 <- list("utterances" = utterances2, "tokens" = tokens2)

  both=list(tibble(get_predictors(half1, lang)), tibble(get_predictors(half2, lang)))
  return(both)  
  }
#######################   


# adjust with spearman-brown formula
sbformula <- function(r){
  if (!is.na(r)){
  r1<-(2*r)/(1+r)
  }else{
  r1=NA
  }
  return(r1)
}

#######################  

# measure correlation between two vectors
pred_cor <- function(pred, df){
r<- cor(num_zero(df[[1]][[pred]]), num_zero(df[[2]][[pred]]), method="pearson")
#return(r)  
}

#######################  

# replace NA with 0
num_zero <- function(vector_with_nas) {
  vector_with_nas[is.na(vector_with_nas)] <- 0
  return(as.numeric(vector_with_nas))
}
#######################

# main function for reliabilities of predictors within childes
get_main_rel <-function(lang){
  both<-split_half(lang)
  df<- same_size_df(both[[1]],both[[2]])
  r=c(pred_cor("length_char", df),pred_cor("n_tokens", df), pred_cor("mlu", df),  pred_cor("frequency", df), pred_cor("solo_frequency", df), pred_cor("first_frequency", df), pred_cor("final_frequency", df))
  names= predictor_list
  rdata <- data.frame(names, r)
  rdata <- rdata %>% mutate(language=lang)
  return(rdata)
}

######################

# add lemmas of one childes half not existing at other half and do same size and order
same_size_df <- function(df1, df2) { 
 # if (!is.na(df1)){
  if (nrow(df1)>3){
  firstlistlemma<-(df1$uni_lemma)
  secondlistlemma<-(df2$uni_lemma)
  diff1<-setdiff(firstlistlemma,secondlistlemma) 
  diff2<-setdiff(secondlistlemma,firstlistlemma) 
  df<-as.data.frame(c(diff1, diff2))
  colnames(df)<- c("uni_lemma")
  add_missing <- function(df1, df){
      df1<- df1 %>%
      full_join(df) %>%
      arrange(uni_lemma)
      return(df1)
      }
    df1<- add_missing(df1, df)
    df2<- add_missing(df2, df)
  both=list(tibble(df1[!duplicated(df1$uni_lemma),]), tibble(df2[!duplicated(df2$uni_lemma),]))
  return(both)
  }
  else{
  both=list(tibble(df1), tibble(df2))
  return(both)
  }
}

##########################

# extracts corr number from df
lang_pred_half <- function(lang, pred, split_corpora){
 s=split_corpora %>% filter(language==lang, names==pred)
 r=s[1,2]
return(as.numeric(r) ) 
}

#############

# measures aoa for each item
measure_aoas <- function(admin_summary) {
  d<- admin_summary |>
  mutate(num_false = total - num_true,
         prop = num_true / total) |>
  dplyr::select(language, measure, uni_lemma, prop, num_true,age, num_false, total, items) %>%
         unique()
  admin_aoas <- d|>
  group_by(language, measure) |>
  nest() |>
  mutate(aoa = map(data, get_aoas, max_steps = 400)) |>
  dplyr::select(-data) |>
  unnest(cols = c(aoa))
  return(admin_aoas)
 }
################################


null_items <- function(){
return(NULL)
}

#######################


# get wordbank and randomly split in two
split_wordbank <- function(lang, form, meas){
  items <- tryCatch({items <- create_inst_data(lang, form) %>%
     filter(measure==meas)
    }, error= function(e){})
   if (length(unique(items$data_id)) >1){ #if at least 1 administration
   admin<-as.data.frame(unique(items$data_id))
   ind <- sample(c(TRUE, FALSE), nrow(admin), replace=TRUE, prob=c(0.5, 0.5))
   print(glue("Randomly split administrations..."))
   adminfirstnum <- admin[ind, ]
   adminfirst<-items %>% filter(data_id %in% adminfirstnum)
   adminfirst_summary <- collapse_inst_data(adminfirst)
   
   adminsecondnum <- admin[!ind, ] #create two groups of administrations
   adminsecond<-items %>% filter(data_id %in% adminsecondnum)
   adminsecond_summary <- collapse_inst_data(adminsecond)
   
   df <- list(adminfirst_summary, adminsecond_summary)
   } 
   else{
   df<- NULL
    }
return(df)
}

#################  

# main function for reliabilities of aoa within wordbank
get_main_rel_aoa <-function(lang, meas){
  print(glue("Get CDI item data for {lang} ..."))
  if (is.null(nrow(split_wordbank(lang, "WS", meas)[[1]]))){
    r=NA
    } 
  else if (is.null(nrow(split_wordbank(lang, "WG", meas)[[1]]))){
    first_aoas <- measure_aoas(split_wordbank(lang, "WS", meas)[[1]])
    second_aoas <-measure_aoas(split_wordbank(lang, "WS", meas)[[2]])
    r=cor(first_aoas$aoa, second_aoas$aoa, use="complete.obs", method="pearson")
    } 
  else{   
  first_summary <- combine_form_data(list(split_wordbank(lang, "WS", meas)[[1]], split_wordbank(lang, "WG", meas)[[1]]))
  second_summary <- combine_form_data(list(split_wordbank(lang, "WS", meas)[[2]], split_wordbank(lang, "WG", meas)[[2]]))
  first_aoas <- measure_aoas(first_summary)
  second_aoas <-measure_aoas(second_summary)
  r<-cor(first_aoas$aoa, second_aoas$aoa, use="complete.obs", method="pearson") #measure r
    }
  return(r)
}

```
### Measure reliabilities for aoa WordBank
```{r aoas}

reliabilities_aoa <- expand_grid(lang = target_langs,
                                 meas = c("produces", "understands"),
                            word_class = c("all")) %>% # 
  rowwise %>%
  mutate(split_half_aoa = ifelse(is.na(get_main_rel_aoa(lang, meas)), NA, (sum(get_main_rel_aoa(lang, meas), get_main_rel_aoa(lang, meas), get_main_rel_aoa(lang, meas),get_main_rel_aoa(lang, meas),get_main_rel_aoa(lang, meas)))/5),
          split_half_aoa_sb = sbformula(split_half_aoa))
  #mutate(split_half_aoa = ifelse(word_class == "all", 
  #                           split_half_cor_aoa(language, measure),
  #                           split_half_cor_aoa(language, word_class)),
        

#pathrelaoa<- glue("{childes_path}reliabilities_aoa.rds")
#  saveRDS(reliabilities_aoa, pathrelaoa)
reliabilities_aoa %>%
  knitr::kable(digits = 2)

```

### Measure reliabilities for CHILDES predictors
```{r childes}
  
split_corpora <- lapply(target_langs, get_main_rel) %>% bind_rows()

split_corpora1 <- lapply(target_langs, get_main_rel) %>% bind_rows()

split_corpora2 <- lapply(target_langs, get_main_rel) %>% bind_rows()

split_corpora3 <- lapply(target_langs, get_main_rel) %>% bind_rows()

split_corpora4 <- lapply(target_langs, get_main_rel) %>% bind_rows()

#splitpath<- glue("{childes_path}split_corpora.rds")#  saveRDS(split_corpora, splitpath)

reliabilities <- expand_grid(lang = target_langs,
                            word_class = c("all"),
                            pred = predictor_list) %>%  
                rowwise %>%            
 mutate(corrr =  (sum(lang_pred_half(lang, pred, split_corpora), lang_pred_half(lang, pred, split_corpora1), lang_pred_half(lang, pred, split_corpora2), lang_pred_half(lang, pred, split_corpora3), lang_pred_half(lang, pred, split_corpora4)))/5) %>% 
  mutate(sbr = sbformula(as.numeric(corrr)))

reliabilities %>%
  knitr::kable(digits = 2)



```

### Measure R squared from cross-validation 
```{r x-validation}

rsq_cv<- function(word_values, predictor, lang, meas) {
  r<-word_values |>
  filter(language==lang, measure==meas) |>
  group_nest() |>
  mutate(loo_data = map(data, crossv_loo),
         loo_models = map(data, fit_cv_models, list(make_effs_formula(predictor, FALSE, TRUE))),
         loo_preds = map2(loo_models, data, get_cv_preds),
         cv_results = map(loo_preds, get_cv_results))
   
  r_lp<- r |>
    unnest(cv_results) %>% 
    dplyr::select(r2) %>%
   unique()
  r=r_lp[1,1]
  return(as.character(r)) 
  }


word_values <- read.csv("~/Desktop/word_values.csv") ####data needed from aoa-template

rsq <- expand_grid(  #     class = c("all"),
                   pred = predictor_list,
                   lang = target_langs,
                   meas = c("produces", "understands")) %>% 
  rowwise %>%
  mutate(r2= rsq_cv(word_values, pred, lang, meas))

```

### Merge to create final corpus 
```{r final corpus}

dfinal <- reliabilities %>%
  left_join(reliabilities_aoa) %>%
    unique() %>%
   filter(!is.na(meas)) %>% 
   filter(!is.na(corrr)) %>%
   filter(!is.na(split_half_aoa)) %>%
    mutate(threshold_half = sbr * split_half_aoa_sb) %>%
    left_join(unique(rsq)) %>%
  filter(!is.na(r2)) %>% 
   filter(!is.infinite(as.numeric(r2))) %>%
  unique()


dfinal %>%
  knitr::kable(digits = 2)

#saveRDS(dfinal, "data/reliabilities.rds")

```

### Build final plot 
```{r final_Data}

relia_plot <- function(dfinal, lang_list){
 ggplot() +  
  geom_errorbar(dfinal, mapping=aes(x=pred, y=threshold_half, ymax=threshold_half, ymin=threshold_half,  color=pred))+
  geom_bar(dfinal,mapping=aes(x=pred, y=as.numeric(r2), fill=pred), position="dodge", stat="identity")  +
  theme(axis.text.x = element_text(angle = 30))+
  facet_grid(lang ~ meas) +
  theme(legend.position = "bottom") +
  ylab("Predictor") +
  xlab("R2") +
  ylim(-1,1)+  
  theme(legend.title = element_blank())+
  theme_bw()
}

relia_plot(dfinal, target_langs)


```
```{r reliability_alpha, include=FALSE}

# cronbach_alpha <-function(dataAoa, corpus_frequency_){ 
#  lang_str <- normalize_language(lang) 
#   corpus <- read_csv(glue("data/childes/childes_tokens_{lang_str}.csv"))
#   corpus1 <- corpus %>% 
#     filter(gloss %in% wblemmas) %>% 
#       group_by(gloss, target_child_id) %>% 
#          summarize(count=n())  
#   corpus2 <- corpus %>%  
#     group_by(target_child_id) %>% 
#      summarize(total=n()) 
#   corpus <- corpus1 %>% 
#    left_join(corpus2) %>% 
#       mutate(freq=count/total) 

# corpus_ <- corpus %>% 
#        mutate(stem = stem(gloss, convert_stemlang(lang))) %>% 
#         full_join(load_unilemmas() %>% filter(language == normalize_language(lang)),
# by = "stem") %>% -->
#           group_by(uni_lemma, target_child_id) %>% 
#             mutate(FreqLemma = sum(freq, na.rm = TRUE))#

#   lemma_<-corpus_$uni_lemma  #restructure dataframe 
#   target_child_id_<-corpus_$target_child_id 
#   freq_<-corpus_$FreqLemma 

#   df<-unique(data.frame(lemma_, target_child_id_, freq_))
#   corpus<-tidyr::spread(df, target_child_id_, freq_) 

#  child_ids_<- unique(as.character(colnames(corpus)[3:ncol(corpus)])) 
#  child<-select(corpus, child_ids_ ) 
#   a<-alpha(child) 
#   return(a$total[1,1])  

```


