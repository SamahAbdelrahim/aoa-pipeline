---
title: "aoa-pred-surprisal"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
options(dplyr.summarise.inform = FALSE)

# load libraries
library(tidyverse)
library(glue)
library(wordbankr)
#install.packages("remotes")
#remotes::install_github("langcog/childesr")
library(childesr)
library(broom)
library(car)
#library(jglmm)
library(modelr)
library(ggrepel)
library(SnowballC)
library(stringr)
#library(Hmisc)
#library(arm)

# load functions
setwd("~/Documents/LanguageLearning/aoa-pipeline/")
walk(list.files("scripts", pattern = "*.R$", full.names = TRUE), source)
```

# Load Wordbank data

Loading cached Wordbank data for multiple languages:
```{r loadwordbankxling}
target_langs <- c("French (Quebecois)", "German", "English (American)", "Spanish (Mexican)","Mandarin (Beijing)", "French (French)", "English (Australian)", "English (British)", "Mandarin (Taiwanese)", "Spanish (European)" )



wb_data <- map_df(target_langs, function(lang) {
  print(glue("Loading data for {lang}..."))
  norm_lang <- normalize_language(lang)
  tryCatch( 
    {
      # If data for language X is already cashed, it will be loaded directly into the workspace
      readRDS(glue("../data/wordbank/{norm_lang}.rds"))
    },
    error = function(e) {
      # If the data for language X is not cashed, it will download it for all available instruments types, cashe the data for future use and then load it into the workspace
      print(glue("No cashed data for {lang}, downloading data now..."))
      create_wb_data(lang)
      readRDS(glue("../data/wordbank/{norm_lang}.rds"))
    }
    )

})
wb_data

```

# Get word lists and uni_lemmas for "Mandarin (Taiwanese)" and "Spanish (European)"
These languages have now been cashed so there is no more need to run this. It is included in the event that we need to do this again for other language.

There are currently (September 2021) no uni-lemmas in Wordbankr for "Mandarin (Taiwanese)" and "Spanish (European)", however we do have them in development. Here we combine the data from wordbankr and the uni-lemmas in development for them.

```{r new_uni_lemmas, eval=FALSE}

no_uni_lemma_langs <- c("Mandarin (Taiwanese)", "Spanish (European)")
# No unilemmas for "Mandarin (Taiwanese)" and "Spanish (European)" 

get_uni_lemmas_from_csv <- function(lang){
  lang_norm <- normalize_language(lang)
  name <- glue("../word_lists/Wordbank_{lang_norm}.csv")
  new_uni_lemmas <- read_csv(name)
  
  new_uni_lemmas <- new_uni_lemmas |>
    select(-c(gloss, new_gloss, new_uni_lemma, notes)) |>
    filter(!is.na(uni_lemma))
  
  ws_new_uni_lemmas <- new_uni_lemmas |>
    mutate(item_id = WS) |>
    select(-c(WG, WS)) |>
    filter(!is.na(item_id)) |>
    mutate(form = "WS")
  
  wg_new_uni_lemmas <- new_uni_lemmas |>
    mutate(item_id = WG) |>
    select(-c(WG, WS)) |>
    filter(!is.na(item_id)) |>
    mutate(form = "WG")
  
  new_uni_lemmas <- rbind(ws_new_uni_lemmas, wg_new_uni_lemmas) |>
    mutate(language = lang)
  
  lang_wg <- create_inst_data(lang, "WG") |>
    select(-c(uni_lemma)) |>
    left_join(new_uni_lemmas)

  lang_ws <- create_inst_data(lang, "WS") |>
    select(-c(uni_lemma)) |>
    left_join(new_uni_lemmas)
  
  wg_summary <- collapse_inst_data(lang_wg)
  ws_summary <- collapse_inst_data(lang_ws)
  comb_summary <- combine_form_data(list(wg_summary, ws_summary))
  
  saveRDS(comb_summary, file = glue("data/wordbank/{lang_norm}.rds"))
  
  return(comb_summary)
}

wb_data_new_unilemmas <- map_df(no_uni_lemma_langs, get_uni_lemmas_from_csv)

```

# Make word lists for each language

Get match unilemmas for each item
```{r get_word_lists}

get_word_lists <- function(wb_data) {
  split_words <- wb_data |>
    distinct(language, uni_lemma, items) |>
    unnest(items) |>
    select(-c(form, item_id)) |>
    filter(lexical_class != "other") |>
    distinct() |>
    mutate(word_clean = gsub(" [(].*$","", definition)) |>
    mutate(word_clean = gsub("[(].*$","", definition)) |>
    mutate(word_clean = gsub(" $","", definition)) |>
    separate(col=word_clean, into=c("word_clean1", "word_clean2", "word_clean3"), sep="/")
  words <- split_words |>
    mutate(word_clean = word_clean1) |>
    select(-c( word_clean1, word_clean2, word_clean3))
  words2 <- split_words |>
    mutate(word_clean = word_clean2) |>
    select(-c( word_clean1, word_clean2, word_clean3)) |>
    filter(! is.na(word_clean))
  words3 <- split_words |>
    mutate(word_clean = word_clean3) |>
    select(-c( word_clean1, word_clean2, word_clean3)) |>
    filter(! is.na(word_clean))  
  word_lists <- rbind(words, words2, words3)
}

word_lists <- get_word_lists(wb_data)

```

```{r save_word_list_by_lang}
save_lang_word_list <- function(lang){
  lang_word_list <- word_lists |>
    filter(language == lang)
  norm_lang <- normalize_language(lang)
  name <- glue("../word_lists/word_list_{norm_lang}.csv")
  write.csv(lang_word_list, file=name, sep=",")
  return(lang_word_list)
}

map_df(target_langs, save_lang_word_list)

```


# Load predictors

Merge in the by-concept predictors (concreteness) to the unilemmas.

```{r merge_unilemmas}
#uni_lemmas <- get_uni_lemmas(wb_data)
#uni_lemma_map <- build_uni_lemma_map(uni_lemmas)
uni_lemmas <- extract_uni_lemmas(wb_data)
```

```{r load_predictors}
setwd("~/Documents/LanguageLearning/aoa-pipeline/")
concreteness_map <- c(word = "Word", concreteness = "Conc.M")
concreteness <- uni_lemmas |> map_predictor("concreteness", concreteness_map)
```

# Load suprisal and frequency 

```{r load_freq}
readRDS("./surprisals/frequencies.rds")
frequencies <- frequencies |> select(-c(n_train_instances, n_val_instances, n_total, train_frequency))
```

Load surprisal and perplexity values
```{r load_model_surprisals}
readRDS("./surprisals/lstm_surprisals.rds")
surprisals <- lstm_surprisals |> select(-c(n_instances))
```

```{r load_ngram_suprisals}

```

```{r load_finetuned_suprisals}

```


Combine all predictors by unilemma
```{r combine_all}

predictor_data <- surprisals |> left_join(frequencies) |> left_join(concreteness)

```


# Set lexical contrasts and predictors list

```{r lex_contrast}
data_lexcat <- prep_lexcat(predictor_data, uni_lemmas)

predictor_sources <- list(
  "all_frequency",
  c("avg_surprisal", "avg_perplexity"),
  "concreteness"
)
predictors <- unlist(predictor_sources)
```

# Impute data or remove NAs
```{r impute}
data_imputed <- data_lexcat |> 
  do_full_imputation(predictor_sources, max_steps = 20)

data_scaled <- do_scaling(data_imputed, predictors)
```

If not imputing, remove NA data points

```{r remove_NA}

remove_NA_predictors <- function(data, predictors){
  for (pred in predictors){
    data <- data |> filter(!is.na(data[[pred]]))
  }
  return(data)
}

data_no_na <- remove_NA_predictors(data_lexcat, predictors)

data_scaled <- do_scaling(data_no_na, predictors)
```


# Fit models by language
#Get fitted AoAs
```{r aoa-lm}
aoas <- fit_aoas(wb_data)
aoa_predictor_data <- aoas |> left_join(data_lexcat) |> remove_NA_predictors(predictors)

```


If not imputing, remove NA data points

```{r remove_NA}

remove_NA_predictors <- function(data, predictors){
  for (pred in predictors){
    data <- data |> filter(!is.na(data[[pred]]))
  }
  return(data)
}

data_no_na <- remove_NA_predictors(data_lexcat, predictors)

data_scaled <- do_scaling(data_no_na, predictors)
```


# Fit models by language
Get fitted AoAs
```{r aoa-lm}
aoas <- fit_aoas(wb_data)
aoa_predictor_data <- aoas |> left_join(data_lexcat) |> remove_NA_predictors(predictors)
```

fit models 
```{r fit_model}
#Choose between avg_surprisal and avg_perplexity. can't use both since in some languages they are highly correlated (English, German)
predictors <- c("avg_surprisal", "all_frequency", "concreteness")
#predictors <- c("avg_perplexity", "all_frequency", "concreteness")
aoa_models <- fit_models(predictors, aoa_predictor_data)
```
Results 

Coefficients:
```{r}
coefs_data <- aoa_models |> select(language, measure, coefs) |> unnest(coefs)
```

Summary stats:
```{r}
stats_data <- aoa_models |> select(language, measure, stats) |> unnest(stats)
```

Variance inflation factors:
```{r}
vifs_data <- aoa_models |> select(language, measure, vifs) |> unnest(vifs)
```

# Cross-validation

```{r cross_validate}

all_full = ~ lexical_category * avg_surprisal + lexical_category * all_frequency + lexical_category * concreteness
freq_full = ~ lexical_category * all_frequency + lexical_category * concreteness
freq_only = ~ lexical_category * all_frequency
surp_full = ~ lexical_category * avg_surprisal + lexical_category * concreteness
surp_only = ~ lexical_category * avg_surprisal
null_model = ~ 1
formulae <- formulas(~aoa, null_model, all_full, freq_only, freq_full, surp_only, surp_full)


# for simple cv result extraction
loo_df <- aoa_predictor_data |>
  group_nest(language, measure) |>
  mutate(loo_models =  map(data, fit_cv_models, formulae),
         loo_preds = map2(loo_models, data, get_cv_preds),
         cv_results = map(loo_preds, get_cv_results))


cv_results <- loo_df |>
  select(language, measure, cv_results) |>
  unnest(cv_results)

loo_preds <- loo_df |>
  select(language, measure, loo_preds, data)

cv_results_pos <- loo_preds |>
  unnest(loo_preds) |> 
  group_by(language, measure, lexical_category) |>
  summarise(mean_abs_dev = mean(abs_dev), mean_se = mean(se))

cv_results_cat <- loo_preds |>
  unnest(c(loo_preds, data), names_repair = "unique") |>
  group_by(language, measure, category) |>
  summarise(mean_abs_dev = mean(abs_dev), mean_se = mean(se))

cv_results_lex <- loo_preds |>
  unnest(cols = c(loo_preds))|>
  group_by(language, measure, lexical_category) |>
  summarise(mean_abs_dev = mean(abs_dev), mean_se = mean(se))

eng_across_lang_lex_desc <- loo_preds |>
  unnest(cols = c(loo_preds))|>
  filter(language == "English (American)", measure == "produces") |>
  arrange(desc(abs_dev)) |>
  head(50)
```

