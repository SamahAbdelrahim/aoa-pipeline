---
title: "AoA prediction template"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
options(dplyr.summarise.inform = FALSE)

# load libraries
library(tidyverse)
library(glue)

# load functions
walk(list.files("scripts", pattern = "*.R", full.names = TRUE), source)

# set up julia
library(jglmm)
options(JULIA_HOME = "/Applications/Julia-1.5.app/Contents/Resources/julia/bin")
jglmm_setup()
```

This template provides the structure for how to fit age of acquisition (AoA) prediction models, using data and scripts in this repository.

The general steps are:
- loading the provided CDI data and predictor data
- adding your predictor(s) to the provided predictor data
- using the functions in `scripts/prep_data.R` to prepare the data for modeling
- using the functions in `scripts/fit_models.R` to fit models and extract information from them


# Wordbank data

Loading cached Wordbank data for English:
```{r}
eng_data <- readRDS("data/wordbank/english_(american).rds")
eng_data
```

Loading cached Wordbank data for multiple languages:
```{r}
#target_langs <- c("Croatian", "Danish", "English (American)",
          #        "French (Quebecois)", "Italian", "Norwegian", "Russian",
           #       "Spanish (Mexican)", "Swedish", "Turkish")
target_langs <- c("Spanish (Mexican)")


lang_data <- map_df(target_langs, function(lang) {
  message(glue("Loading data for {lang}..."))
  lang_str <- normalize_language(lang)
  readRDS(glue("data/wordbank/{lang_str}.rds"))
})

```

Creating saved Wordbank data for English:
```{r, eval=FALSE}
create_lang_data("English (American)")
```

Creating saved Wordbank data for many languages:
```{r, eval=FALSE}
walk(target_langs, create_lang_data)

```

Creating saved Wordbank data one step at a time (potentially making changes between steps):
```{r}
eng_wg <- create_inst_data("English (American)", "WG")
eng_ws <- create_inst_data("English (American)", "WS")
eng_wg_summary <- collapse_inst_data(eng_wg)
eng_ws_summary <- collapse_inst_data(eng_ws)
eng_summary <- combine_form_data(list(eng_wg_summary, eng_ws_summary))
eng_summary
```

```{r}
# quick viz of item trajectories
# lang_data %>%
#   mutate(prop = num_true / total,
#          measure = fct_relevel(measure, "understands")) %>%
# ggplot(aes(x = age, y = prop)) +
#   facet_grid(cols = vars(measure), rows = vars(language),
#              scales = "free_x", space = "free_x",
#              labeller = as_labeller(str_to_sentence)) +
#   geom_smooth(aes(colour = uni_lemma, weight = total), alpha = 0.1, size = 0.1,
#               se = FALSE, method = "glm", method.args = list(family = "binomial")) +
#   scale_colour_discrete(guide = FALSE) +
#   scale_x_continuous(breaks = seq(8, 30, 4)) +
#   labs(x = "Age (months)", y = "Proportion of children")
```


# Predictors

Merge in the by-concept predictors (babiness, concreteness, etc) to the unilemmas and the by word predictors (phonemes) to the words/definitions.

```{r}
uni_lemmas <- lang_data %>%
  distinct(language, uni_lemma, items) %>%
  unnest(items) %>%
  select(-form) %>%
  nest(items = -c(language, uni_lemma))

babiness_map <- c(word = "word", babiness = "babyAVG")
babiness <- uni_lemmas %>% map_predictor("babiness", babiness_map)

valence_map <- c(word = "Word", valence = "V.Mean.Sum", arousal = "A.Mean.Sum")
valence <- uni_lemmas %>% map_predictor("valence", valence_map)

concreteness_map <- c(word = "Word", concreteness = "Conc.M")
concreteness <- uni_lemmas %>% map_predictor("concreteness", concreteness_map)

```

```{r}
# TODO: requires espeak
# http://espeak.sourceforge.net

phonemes <- uni_lemmas %>% map_phonemes()

uni_joined <- uni_lemmas %>% 
  # left_join(phonemes, by = c("language", "uni_lemma")) %>%
  left_join(babiness, by = c("language", "uni_lemma")) %>% 
  left_join(valence, by = c("language", "uni_lemma")) %>% 
  left_join(concreteness, by = c("language", "uni_lemma"))
```

Add CHILDES predictors

```{r}
## A more fine-grained search can be done for each language with this command: childes_predictors<- get_childes_metrics(lang, corpus, word, speaker_role, speaker_role_exclude, target_child, child_age = NULL, child_sex, uttlength=TRUE, freq=TRUE, charlen=TRUE, order=TRUE, clean=FALSE)

childes_predictors <- map_df(target_langs, get_childes_metrics) 

childes_predictors <- childes_predictors %>%
  # ungroup() %>%
  distinct(uni_lemma, MLU, frequency, solo_frequency, initial_frequency,
           final_frequency, nb_realisations_lemma, mean_character_count_lemma,
           totalcount)

uni_joined_ <- uni_joined %>%
  left_join(clean_childes_predictors)
```


Residualize solo freq and final freq

```{r}


```


```{r impute}
prepped_data <- uni_joined %>%
  #select out just the by lexical item data
  unnest(cols = "items") %>%
  select(-c(age, num_true, total, form, item_id)) %>%
  distinct() %>%
  #pull out categories from classes
  mutate(lexical_category = if_else(str_detect(lexical_class, ","), "other", lexical_class),
         # collapse v, adj, adv into one category
         lexical_category = lexical_category %>% as_factor() %>% 
           #TODO: What to do if the languages selected don't have one of these - cant use fct collapse?
           fct_collapse("predicates" = c("verbs", "adjectives", "adverbs"))) %>%
  select(-lexical_class)

pred_sources <- list(
  #c("frequency", "MLU", "final_frequency", "solo_frequency"),
  c("valence", "arousal"), 
  "concreteness", "babiness", "num_phons"
)

imputed_data <- prepped_data %>% do_full_imputation(pred_sources, 20)
```
